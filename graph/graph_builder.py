import os
import re
import json
import logging
import networkx as nx
from typing import List, Dict, Tuple, Any
from collections import defaultdict
from llm.llm import LLMClient
from networkx.readwrite import json_graph
from utils.io import load_json, save_graph
from graph.entity_extractor import EntityExtractor, extract_relations_with_llm
from graph.performance_monitor import monitor_performance, memory_management_context, check_gpu_memory_usage, optimize_for_memory

@monitor_performance("å®ä½“å…³ç³»æŠ½å–")
def extract_entities_and_relations(text: str, llm_client: LLMClient, entity_extractor: EntityExtractor = None, config: dict = None):
    """
    ä½¿ç”¨æ”¹è¿›çš„å®ä½“æŠ½å–å™¨å’ŒLLMæŠ½å–å®ä½“ä¸å…³ç³» - ä¼˜åŒ–ç‰ˆæœ¬
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        llm_client: LLMå®¢æˆ·ç«¯
        entity_extractor: å®ä½“æŠ½å–å™¨å®ä¾‹
        config: é…ç½®ä¿¡æ¯
        
    Returns:
        å…³ç³»ä¸‰å…ƒç»„åˆ—è¡¨ (å¤´å®ä½“, å…³ç³», å°¾å®ä½“)
    """
    if entity_extractor is None:
        # æ­£ç¡®åˆå§‹åŒ–EntityExtractorï¼Œä¼ é€’å®Œæ•´é…ç½®
        entity_extractor = EntityExtractor(config)
    
    try:
        # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        if check_gpu_memory_usage():
            optimize_for_memory()
            
        # æ–‡æœ¬é¢„å¤„ç†ï¼Œç§»é™¤è¿‡å¤šç©ºç™½å­—ç¬¦
        text = ' '.join(text.split())
        
        # 1. ä½¿ç”¨æ”¹è¿›çš„å®ä½“æŠ½å–å™¨è¯†åˆ«å®ä½“
        with memory_management_context("å®ä½“æŠ½å–"):
            entities_info = entity_extractor.extract_entities(text)
            entities = [entity['text'] for entity in entities_info]
        
        # è·å–æœ€å¤§å®ä½“æ•°é‡é™åˆ¶
        max_entities_per_topic = config.get('graph_construction', {}).get('max_entities_per_topic', 50) if config else 50
        
        # é™åˆ¶å®ä½“æ•°é‡
        if len(entities) > max_entities_per_topic:
            # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œä¿ç•™æœ€é«˜ç½®ä¿¡åº¦çš„å®ä½“
            entities_with_conf = sorted(entities_info, key=lambda x: x.get('confidence', 0), reverse=True)[:max_entities_per_topic]
            entities = [entity['text'] for entity in entities_with_conf]
            print(f"[âš ï¸ å®ä½“æ•°é‡é™åˆ¶]: {len(entities)}/{max_entities_per_topic}")
        
        if len(entities) < 2:
            print(f"[âš ï¸ å®ä½“æ•°é‡ä¸è¶³]: ä»…è¯†åˆ«åˆ° {len(entities)} ä¸ªå®ä½“")
            return []
        
        print(f"[âœ… å®ä½“è¯†åˆ«]: è¯†åˆ«åˆ° {len(entities)} ä¸ªå®ä½“: {entities[:10]}{'...' if len(entities) > 10 else ''}")
        
        # 2. ä½¿ç”¨LLMæŠ½å–å®ä½“é—´çš„å…³ç³»ï¼ˆé™åˆ¶å®ä½“æ•°é‡ä»¥æé«˜æ•ˆç‡ï¼‰
        if len(entities) <= 20:  # é™åˆ¶LLMå¤„ç†çš„å®ä½“æ•°é‡
            with memory_management_context("å…³ç³»æŠ½å–"):
                # å†æ¬¡æ£€æŸ¥GPUå†…å­˜
                if check_gpu_memory_usage():
                    print(f"[âš ï¸ GPUå†…å­˜ä¸è¶³]: è·³è¿‡LLMå…³ç³»æŠ½å–")
                    triples = []
                else:
                    triples = extract_relations_with_llm(text, entities, llm_client)
        else:
            print(f"[âš ï¸ å®ä½“æ•°é‡è¿‡å¤š({len(entities)})]: è·³è¿‡LLMå…³ç³»æŠ½å–")
            triples = []
        
        # è¿‡æ»¤è‡ªç¯å…³ç³»
        filtered_triples = []
        for triple in triples:
            if len(triple) == 3:
                head, rel, tail = triple
                if head.lower() != tail.lower():  # è·³è¿‡è‡ªç¯å…³ç³»
                    filtered_triples.append(triple)
        
        print(f"[âœ… å…³ç³»æŠ½å–]: æŠ½å–åˆ° {len(filtered_triples)} ä¸ªå…³ç³»ä¸‰å…ƒç»„")
        
        return filtered_triples
        
    except Exception as e:
        print(f"âŒ å®ä½“å…³ç³»æŠ½å–å¤±è´¥: {e}")
        return []


def build_graph(chunks: list[dict], llm_client: LLMClient, config: dict = None, logger=None):
    """
    æ„å»ºçŸ¥è¯†å›¾è°±
    
    Args:
        chunks: æ–‡æ¡£å—åˆ—è¡¨
        llm_client: LLMå®¢æˆ·ç«¯
        config: é…ç½®ä¿¡æ¯
        
    Returns:
        æ„å»ºçš„çŸ¥è¯†å›¾è°±
    """
    G = nx.DiGraph()
    
    # å¦‚æœæ²¡æœ‰æä¾›loggerï¼Œä½¿ç”¨é»˜è®¤çš„printè¾“å‡º
    if logger is None:
        class DefaultLogger:
            def warning(self, msg): print(f"WARNING: {msg}")
            def info(self, msg): print(f"INFO: {msg}")
            def error(self, msg): print(f"ERROR: {msg}")
        logger = DefaultLogger()
    
    # åˆå§‹åŒ–å®ä½“æŠ½å–å™¨å’Œå›¾æ„å»ºé…ç½®
    graph_config = config.get("graph_construction", {}) if config else {}
    entity_extractor = EntityExtractor(config)
    
    # å›¾æ„å»ºå‚æ•°
    enable_reverse_links = graph_config.get("enable_reverse_links", True)
    max_entities_per_topic = graph_config.get("max_entities_per_topic", 50)
    relation_confidence_threshold = graph_config.get("relation_confidence_threshold", 0.6)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_entities = set()
    total_relations = 0
    
    for i, chunk in enumerate(chunks):
        # å¤„ç†ä¸åŒæ•°æ®æ ¼å¼çš„å…¼å®¹æ€§
        if "text" in chunk:
            text = chunk["text"]
        elif "sentences" in chunk:
            # å¤„ç†static_chunk_processorç”Ÿæˆçš„æ ¼å¼
            text = "\n".join(chunk["sentences"]) if isinstance(chunk["sentences"], list) else str(chunk["sentences"])
        else:
            logger.warning(f"å— {i} ç¼ºå°‘æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡å¤„ç†")
            continue
            
        chunk_id = chunk["id"]
        summary = chunk.get("summary", chunk_id)
        topic_node_id = f"topic::{chunk_id}"

        # åˆ›å»ºä¸»é¢˜èŠ‚ç‚¹ï¼Œæ·»åŠ æ›´å¤šå±æ€§
        G.add_node(topic_node_id, 
                  type="topic", 
                  label=summary,
                  chunk_id=chunk_id,
                  topic_id=chunk_id,  # æ·»åŠ topic_idå±æ€§ç”¨äºå›¾è°±æ£€ç´¢
                  text_length=len(text))

        # æŠ½å–å®ä½“ä¸‰å…ƒç»„
        triples = extract_entities_and_relations(text, llm_client, entity_extractor, config)
        
        if not triples:
            print(f"[âš ï¸ å— {i+1}/{len(chunks)}]: æœªæŠ½å–åˆ°æœ‰æ•ˆå…³ç³»")
            continue
            
        print(f"[ğŸ“Š å— {i+1}/{len(chunks)}]: æŠ½å–åˆ° {len(triples)} ä¸ªå…³ç³»")
        total_relations += len(triples)

        # è®°å½•å½“å‰å—çš„å®ä½“
        chunk_entities = set()
        
        for head, relation, tail in triples:
            # æ·»åŠ å®ä½“èŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if not G.has_node(head):
                G.add_node(head, type="entity", label=head)
            if not G.has_node(tail):
                G.add_node(tail, type="entity", label=tail)
            
            # è®°å½•å®ä½“
            chunk_entities.add(head)
            chunk_entities.add(tail)
            total_entities.add(head)
            total_entities.add(tail)

            # æ·»åŠ å®ä½“é—´çš„å…³ç³»è¾¹
            if G.has_edge(head, tail):
                # å¦‚æœè¾¹å·²å­˜åœ¨ï¼Œæ›´æ–°å…³ç³»ä¿¡æ¯
                existing_relations = G[head][tail].get('relations', [])
                if relation not in existing_relations:
                    existing_relations.append(relation)
                    G[head][tail]['relations'] = existing_relations
            else:
                G.add_edge(head, tail, relation=relation, relations=[relation])

            # è¿æ¥å®ä½“åˆ°ä¸»é¢˜èŠ‚ç‚¹ï¼ˆåŒå‘é“¾æ¥ï¼‰
            if not G.has_edge(topic_node_id, head):
                G.add_edge(topic_node_id, head, relation="åŒ…å«")
            if not G.has_edge(topic_node_id, tail):
                G.add_edge(topic_node_id, tail, relation="åŒ…å«")
                
            # æ·»åŠ å®ä½“â†’ä¸»é¢˜çš„åå‘é“¾æ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if enable_reverse_links:
                if not G.has_edge(head, topic_node_id):
                    G.add_edge(head, topic_node_id, relation="å±äºä¸»é¢˜")
                if not G.has_edge(tail, topic_node_id):
                    G.add_edge(tail, topic_node_id, relation="å±äºä¸»é¢˜")
        
        # ä¸ºä¸»é¢˜èŠ‚ç‚¹æ·»åŠ å®ä½“è®¡æ•°ä¿¡æ¯å’Œé™åˆ¶æ£€æŸ¥
        entity_count = len(chunk_entities)
        G.nodes[topic_node_id]['entity_count'] = entity_count
        
        # å¦‚æœå®ä½“æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œè®°å½•è­¦å‘Š
        if entity_count > max_entities_per_topic:
            print(f"[âš ï¸ è­¦å‘Š]: ä¸»é¢˜ {topic_node_id} åŒ…å« {entity_count} ä¸ªå®ä½“ï¼Œè¶…è¿‡é™åˆ¶ {max_entities_per_topic}")
            G.nodes[topic_node_id]['entity_overflow'] = True
    
    print(f"[ğŸ¯ å›¾æ„å»ºå®Œæˆ]: æ€»å®ä½“æ•° {len(total_entities)}, æ€»å…³ç³»æ•° {total_relations}")
    return G

# save_graphå‡½æ•°å·²ç§»è‡³utils.ioæ¨¡å—

def run_graph_construction(config: dict, work_dir: str, logger):
    from utils.io import load_chunks
    
    output_dir = os.path.join(work_dir)

    logger.info("åŠ è½½æ–‡æœ¬å—...")
    chunks = load_chunks(work_dir)

    llm_client = LLMClient(config["llm"])

    logger.info("å¼€å§‹å®ä½“å›¾æ„å»º...")
    G = build_graph(chunks, llm_client, config["graph"], logger)

    logger.info(f"å›¾æ„å»ºå®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {len(G.nodes)}, è¾¹æ•°: {len(G.edges)}")
    save_graph(G, output_dir)
    logger.info(f"å›¾å·²ä¿å­˜è‡³ {output_dir}/graph.json å’Œ graph.graphml")
