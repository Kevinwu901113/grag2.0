# G-RAG 统一配置文件
# 整合了所有功能模块的配置选项

# ================================
# LLM 配置
# ================================
llm:
  provider: "ollama"  # 支持: ollama, openai, huggingface
  model_name: "qwen2.5:7b-instruct-fp16"
  host: "http://localhost:11434"
  timeout: 30
  max_retries: 3
  options:
    num_ctx: 32768
    temperature: 0.1
    max_tokens: 4000
  
  # OpenAI 配置（当provider为openai时使用）
  # api_key: "your_api_key_here"
  # openai_api_key: "your_api_key_here"
  # base_url: "https://api.openai.com/v1"
  # openai_base_url: "https://api.openai.com/v1"
  # model: "gpt-3.5-turbo"
  # embedding_model: "text-embedding-ada-002"

# ================================
# 嵌入模型配置
# ================================
embedding:
  provider: huggingface  # 支持: huggingface, ollama, openai
  model_name: "BAAI/bge-m3"  # HuggingFace模型
  device: "cuda"  # 设备: cuda, cpu, auto
  normalize: true  # L2归一化
  dimension: 1024  # 嵌入维度
  cache_embeddings: true
  cache_size: 10000
  batch_size: 16  # 减小批量大小以避免CUDA内存不足
  local_files_only: true  # 离线模式，仅使用本地模型文件
  # CUDA内存优化配置
  max_seq_length: 512  # 限制最大序列长度
  enable_memory_optimization: true  # 启用内存优化
  
  # Ollama配置（当provider为ollama时使用）
  # host: "http://localhost:11434"
  # model_name: "nomic-embed-text:latest"
  
  # OpenAI配置（当provider为openai时使用）
  # api_key: "your_api_key_here"
  # model_name: "text-embedding-ada-002"

# ================================
# 文档处理配置
# ================================
document:
  input_dir: "./data/test"
  processing_mode: "clustered"  # 支持: enhanced, clustered, traditional
  chunk_size: 500
  chunk_overlap: 300
  allowed_types: [".docx", ".json", ".jsonl", ".txt", ".md"]
  similarity_threshold: 0.80
  redundancy_threshold: 0.95
  sentence_level_traditional: true
  sentence_level: true
  min_sentence_length: 10
  max_sentence_length: 1000
  embedding_batch_size: 8  # 减小批量大小以避免CUDA内存不足
  enable_redundancy_filter: true

# ================================
# 聚类文档处理配置
# ================================
document_processing:
  strategy: "clustered"  # 可选：clustered 或 incremental
  chunk_length: 200  # 文本块字符数
  min_chunk_length: 50  # 最小块长度
  cluster_count: 50  # 聚类数量，null 表示自动估算
  auto_estimate_clusters: true  # 是否自动估算聚类数
  min_clusters: 5  # 最小聚类数
  max_clusters: 100  # 最大聚类数
  clustering_method: "kmeans"  # 聚类方法：kmeans 或 faiss
  enable_scaling: false  # 是否对特征进行标准化
  enable_quality_assessment: true  # 是否启用聚类质量评估
  max_chunks_per_block: 100  # 每个主题块最大文本块数
  max_summary_length: 200  # 摘要最大长度
  min_block_size: 2  # 最小主题块大小（文本块数量）
  max_block_size: 50  # 最大主题块大小
  log_cluster_distribution: true  # 是否记录聚类分布
  random_state: 42  # 随机种子
  max_iter: 300  # 最大迭代次数
  n_init: 10  # 初始化次数

# ================================
# 主题池管理配置
# ================================
topic_pool:
  enable_parallel: false
  max_workers: 32
  batch_size: 16  # 减小批量大小以避免CUDA内存不足
  enable_topic_merging: true
  topic_merge_threshold: 0.85
  enable_document_isolation: false
  enable_embedding_cache: true
  embedding_cache_size: 10000

# ================================
# 冗余检测配置
# ================================
redundancy:
  method: "simhash"  # 冗余检测方法: simhash, embedding
  
  # SimHash方法配置（推荐用于大规模数据）
  hamming_threshold: 3  # Hamming距离阈值，越小越严格（推荐1-5）
  max_buffer_size: 100000  # 最大签名缓冲区大小，防止内存溢出
  log_interval: 100  # 每处理多少句子输出一次日志
  enable_logging: true
  enable_progress: true
  
  # 传统embedding方法配置（当method为"embedding"时使用）
  enable_enhanced: false
  similarity_threshold: 0.95
  context_weight: 0.3
  content_weight: 0.7

# ================================
# 知识图谱配置
# ================================
graph:
  enable_graph: true
  entity_extraction_model: "qwen2.5:7b-instruct-fp16"
  relation_extraction_model: "qwen2.5:7b-instruct-fp16"
  max_entities_per_chunk: 30  # 降低以提高性能
  max_relations_per_chunk: 50  # 降低以提高性能
  entity_similarity_threshold: 0.8
  relation_similarity_threshold: 0.7
  # 新增性能优化参数
  batch_size: 8  # 批处理大小
  max_text_length: 800  # 最大文本长度
  enable_parallel_processing: false  # 暂时禁用并行处理避免GPU竞争
  
  # 实体抽取配置
  entity_extraction:
    use_ner_model: true  # 启用NER模型，支持中英文实体抽取
    ner_model_name_en: "dbmdz/bert-large-cased-finetuned-conll03-english"  # 英文NER模型名称
    local_files_only: true  # 允许在线下载模型
    confidence_threshold: 0.6  # 降低阈值以提高召回率和速度
    min_entity_length: 2  # 最小实体长度
    enable_context_validation: false  # 暂时禁用以提高速度
    generic_word_filter: true
    cache_dir: "./models/ner_cache"
    # 新增性能优化参数
    max_text_length: 512  # NER模型最大文本长度
    batch_processing: true  # 启用批处理

# ================================
# 向量索引配置
# ================================
vector:
  index_type: "faiss"
  dimension: 1024
  similarity_metric: "cosine"
  nlist: 100
  nprobe: 10
  enable_gpu: true

# ================================
# 查询处理配置
# ================================
query:
  max_results: 10
  similarity_threshold: 0.7
  enable_reranking: true
  rerank_top_k: 20
  enable_query_expansion: true
  expansion_terms: 3

# ================================
# 输出配置
# ================================
output:
  work_dir: "./result"  # 工作目录，用于保存处理结果
  save_topics: true
  save_graph: true
  save_vectors: true
  output_format: "json"
  compression: false

# ================================
# 性能监控配置
# ================================
performance:
  enable_monitoring: true
  log_memory_usage: true
  log_execution_time: true

# ================================
# 配置说明
# ================================
# 
# 1. LLM提供商选择：
#    - ollama: 本地部署，支持多种开源模型
#    - openai: OpenAI API，需要API密钥
#    - huggingface: HuggingFace模型，主要用于嵌入
#
# 2. 嵌入模型选择：
#    - huggingface (推荐): 本地模型，支持GPU加速，批量处理
#    - ollama: 通过Ollama API调用
#    - openai: OpenAI嵌入API
#
# 3. 文档处理模式：
#    - enhanced: 传统主题池处理，适合中小规模文档
#    - clustered: 静态聚类处理，适合大规模文档
#    - traditional: 基础处理模式
#
# 4. 冗余检测方法选择：
#    - SimHash方法（推荐）：
#      * 适用场景：大规模数据处理（>10000句子）
#      * 性能：比embedding方法快100-1000倍
#      * 内存占用：每句子仅8字节签名
#      * 准确率：95%+（hamming_threshold=3时）
#      * hamming_threshold调优：1（最严格）- 3（推荐）- 5（宽松）- 7+（很宽松）
#    - Embedding方法：
#      * 适用场景：小规模数据或需要最高准确率
#      * 性能：较慢但准确率最高
#      * 内存占用：较大（需存储完整向量）
#
# 5. 性能优化建议：
#    - 大规模数据：使用clustered模式 + simhash冗余检测
#    - 中小规模数据：使用enhanced模式 + embedding冗余检测
#    - GPU可用时：设置device为"cuda"，提升嵌入计算速度
#    - 批量处理：适当增加batch_size，但注意内存限制
#
# 6. 快速开始：
#    - 基础使用：python main.py --config config.yaml
#    - 文档处理：python main.py --mode doc --config config.yaml
#    - 图谱构建：python main.py --mode graph --config config.yaml
#    - 索引构建：python main.py --mode index --config config.yaml
