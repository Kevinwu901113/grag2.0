import torch
import numpy as np
from typing import List, Union
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
import logging
import torch

class HuggingFaceEmbedder:
    """
    HuggingFaceæœ¬åœ°åµŒå…¥æ¨¡å‹å°è£…ç±»
    æ”¯æŒæ‰¹é‡å¤„ç†ã€GPUåŠ é€Ÿå’ŒåµŒå…¥å½’ä¸€åŒ–
    
    æ³¨æ„ï¼šé¦–æ¬¡ä½¿ç”¨æ—¶æ¨¡å‹ä¸‹è½½è¾ƒæ…¢ï¼Œå»ºè®®é¢„å…ˆä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
    """
    
    def __init__(self, model_name: str = "BAAI/bge-m3", device: str = "auto", batch_size: int = 32, local_files_only: bool = False, max_seq_length: int = 512, enable_memory_optimization: bool = True):
        """
        åˆå§‹åŒ–HuggingFaceåµŒå…¥å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨BAAI/bge-m3
            device: è®¾å¤‡ç±»å‹ï¼Œ'cuda', 'cpu' æˆ– 'auto'
            batch_size: æ‰¹é‡å¤„ç†å¤§å°
            local_files_only: æ˜¯å¦ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
            max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦ï¼Œç”¨äºå†…å­˜ä¼˜åŒ–
            enable_memory_optimization: æ˜¯å¦å¯ç”¨å†…å­˜ä¼˜åŒ–
        """
        self.model_name = model_name
        self.batch_size = batch_size
        self.local_files_only = local_files_only
        self.max_seq_length = max_seq_length
        self.enable_memory_optimization = enable_memory_optimization
        
        # è®¾å¤‡é€‰æ‹©
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # åµŒå…¥å½’ä¸€åŒ–é…ç½®
        self.normalize_embeddings = True  # é»˜è®¤å¯ç”¨å½’ä¸€åŒ–
        
        # åµŒå…¥ç»´åº¦ï¼ˆå°†åœ¨é¦–æ¬¡ç¼–ç æ—¶ç¡®å®šï¼‰
        self.embedding_dim = None
            
        print(f"[HuggingFaceEmbedder] åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {model_name}")
        print(f"[HuggingFaceEmbedder] ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"[HuggingFaceEmbedder] æ‰¹é‡å¤§å°: {batch_size}")
        print(f"[HuggingFaceEmbedder] ç¦»çº¿æ¨¡å¼: {local_files_only}")
        
        if not local_files_only:
            print(f"[HuggingFaceEmbedder] æ³¨æ„: é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
        
        try:
            # åŠ è½½sentence-transformersæ¨¡å‹
            self.model = SentenceTransformer(
                model_name, 
                device=self.device,
                local_files_only=local_files_only
            )
            
            # å†…å­˜ä¼˜åŒ–é…ç½®
            if self.enable_memory_optimization and hasattr(self.model, '_modules'):
                # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
                if hasattr(self.model, 'max_seq_length'):
                    self.model.max_seq_length = self.max_seq_length
                    print(f"[HuggingFaceEmbedder] è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦: {self.max_seq_length}")
                
                # å¯ç”¨CUDAå†…å­˜ä¼˜åŒ–
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
                    print(f"[HuggingFaceEmbedder] å·²æ¸…ç†CUDAç¼“å­˜")
            
            # è·å–åµŒå…¥ç»´åº¦
            test_embedding = self.model.encode(["test"], convert_to_numpy=True)
            self.embedding_dim = test_embedding.shape[1]
            
            print(f"[HuggingFaceEmbedder] æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"[HuggingFaceEmbedder] åµŒå…¥ç»´åº¦: {self.embedding_dim}")
            print(f"[HuggingFaceEmbedder] å†…å­˜ä¼˜åŒ–: {self.enable_memory_optimization}")
        except Exception as e:
            print(f"[HuggingFaceEmbedder] åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            if not local_files_only:
                print(f"[HuggingFaceEmbedder] æç¤º: å¦‚æœç½‘ç»œä¸å¯ç”¨ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼Œç„¶åè®¾ç½® local_files_only=True")
            raise RuntimeError(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        

    
    def encode(self, text_list: Union[str, List[str]]) -> List[List[float]]:
        """
        å¯¹æ–‡æœ¬åˆ—è¡¨è¿›è¡ŒåµŒå…¥ç¼–ç 
        
        Args:
            text_list: å•ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²æˆ–æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨ï¼Œæ ¼å¼ä¸º List[List[float]]
            å¦‚æœå¯ç”¨å½’ä¸€åŒ–ï¼Œè¿”å›çš„å‘é‡å·²è¿›è¡ŒL2å½’ä¸€åŒ–ï¼Œé€‚ç”¨äºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
        """
        # ç»Ÿä¸€å¤„ç†è¾“å…¥æ ¼å¼
        if isinstance(text_list, str):
            text_list = [text_list]
        elif not isinstance(text_list, list):
            raise ValueError("è¾“å…¥å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨")
            
        if not text_list:
            raise ValueError("è¾“å…¥æ–‡æœ¬åˆ—è¡¨ä¸ºç©º")
        
        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_texts = [text.strip() for text in text_list if text and text.strip()]
        if not valid_texts:
            raise ValueError("è¿‡æ»¤åæ–‡æœ¬åˆ—è¡¨ä¸ºç©º")
        
        try:
            # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ‰¹é‡ç¼–ç 
            self.logger.debug(f"å¼€å§‹ç¼–ç  {len(valid_texts)} æ¡æ–‡æœ¬ï¼Œæ‰¹é‡å¤§å°: {self.batch_size}")
            
            # å†…å­˜ä¼˜åŒ–ï¼šæˆªæ–­è¿‡é•¿æ–‡æœ¬
            if self.enable_memory_optimization:
                valid_texts = [text[:self.max_seq_length*4] for text in valid_texts]  # ç²—ç•¥ä¼°ç®—å­—ç¬¦æ•°
            
            current_batch_size = self.batch_size
            max_retries = 3
            
            for retry in range(max_retries):
                try:
                    # æ¸…ç†CUDAç¼“å­˜
                    if self.device == "cuda" and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    # SentenceTransformerè‡ªåŠ¨å¤„ç†æ‰¹é‡
                    embeddings = self.model.encode(
                        valid_texts,
                        batch_size=current_batch_size,
                        show_progress_bar=len(valid_texts) > 100,  # å¤§é‡æ–‡æœ¬æ—¶æ˜¾ç¤ºè¿›åº¦æ¡
                        convert_to_numpy=True,
                        normalize_embeddings=False  # æˆ‘ä»¬æ‰‹åŠ¨æ§åˆ¶å½’ä¸€åŒ–
                    )
                    break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                    
                except RuntimeError as cuda_error:
                    if "CUDA out of memory" in str(cuda_error) and retry < max_retries - 1:
                        # CUDAå†…å­˜ä¸è¶³ï¼Œå‡å°æ‰¹é‡å¤§å°é‡è¯•
                        current_batch_size = max(1, current_batch_size // 2)
                        self.logger.warning(f"âš ï¸ CUDAå†…å­˜ä¸è¶³ï¼Œé™ä½æ‰¹é‡å¤§å°è‡³ {current_batch_size} å¹¶é‡è¯• (ç¬¬{retry+1}æ¬¡)")
                        
                        # æ¸…ç†CUDAç¼“å­˜
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        continue
                    else:
                        raise cuda_error
            
            # æ‰‹åŠ¨å½’ä¸€åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.normalize_embeddings:
                embeddings = normalize(embeddings, norm='l2', axis=1)
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            embeddings_list = embeddings.tolist()
            
            self.logger.debug(f"âœ… ç¼–ç å®Œæˆï¼Œç”Ÿæˆ {len(embeddings_list)} ä¸ªåµŒå…¥å‘é‡")
            self.logger.debug(f"å‘é‡ç»´åº¦: {len(embeddings_list[0]) if embeddings_list else 0}")
            if current_batch_size != self.batch_size:
                self.logger.info(f"ğŸ“Š å®é™…ä½¿ç”¨æ‰¹é‡å¤§å°: {current_batch_size} (åŸè®¾ç½®: {self.batch_size})")
            
            return embeddings_list
            
        except Exception as e:
            self.logger.error(f"âŒ åµŒå…¥ç¼–ç å¤±è´¥: {e}")
            # æä¾›å†…å­˜ä¼˜åŒ–å»ºè®®
            if "CUDA out of memory" in str(e):
                self.logger.error("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
                self.logger.error("   1. å‡å°é…ç½®æ–‡ä»¶ä¸­çš„ batch_size (å½“å‰: {})")
                self.logger.error("   2. è®¾ç½® PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")
                self.logger.error("   3. æˆ–å°† device è®¾ç½®ä¸º 'cpu' ä½¿ç”¨CPUå¤„ç†")
            raise RuntimeError(f"åµŒå…¥ç¼–ç å¤±è´¥: {e}")
    
    def get_embedding_dimension(self) -> int:
        """
        è·å–åµŒå…¥å‘é‡ç»´åº¦
        
        Returns:
            åµŒå…¥å‘é‡ç»´åº¦
        """
        return self.embedding_dim
    
    def get_device_info(self) -> dict:
        """
        è·å–è®¾å¤‡ä¿¡æ¯
        
        Returns:
            è®¾å¤‡ä¿¡æ¯å­—å…¸
        """
        return {
            "device": self.device,
            "cuda_available": torch.cuda.is_available(),
            "cuda_device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "model_name": self.model_name,
            "embedding_dim": self.embedding_dim,
            "batch_size": self.batch_size,
            "normalize_embeddings": self.normalize_embeddings
        }