#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜å…ˆçº§æ„ŸçŸ¥ä¸Šä¸‹æ–‡è°ƒåº¦å™¨ (Priority-Aware Context Scheduler)

è¯¥æ¨¡å—å®ç°äº†ä¸€ä¸ªæ™ºèƒ½çš„ä¸Šä¸‹æ–‡è°ƒåº¦æœºåˆ¶ï¼Œæ ¹æ®ç›¸å…³åº¦ã€ç»“æ„æƒé‡å’Œä¸Šä¸‹æ–‡å¤šæ ·æ€§
åŠ¨æ€å†³å®šæ‹¼æ¥å“ªäº›ç‰‡æ®µä½œä¸º LLM è¾“å…¥ä¸Šä¸‹æ–‡ã€‚
"""

import numpy as np
from typing import List, Dict, Any, Tuple
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity

from llm.llm import LLMClient
from utils.common import normalize_scores, improved_tokenize, safe_divide


class PriorityContextScheduler:
    """
    ä¼˜å…ˆçº§ä¸Šä¸‹æ–‡è°ƒåº¦å™¨
    
    æ ¹æ®æ£€ç´¢ç›¸å…³åº¦ã€ç»“æ„æƒé‡å’Œå¤šæ ·æ€§åŠ¨æ€é€‰æ‹©æœ€ä¼˜çš„ä¸Šä¸‹æ–‡ç‰‡æ®µç»„åˆ
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.scheduler_config = config.get('context_scheduler', {})
        
        # è°ƒåº¦å™¨å¼€å…³
        self.enabled = self.scheduler_config.get('enabled', True)
        
        # æƒé‡é…ç½®
        weights = self.scheduler_config.get('weights', {})
        self.relevance_weight = weights.get('relevance', 0.5)  # ç›¸å…³åº¦æƒé‡
        self.structure_weight = weights.get('structure', 0.3)  # ç»“æ„æƒé‡
        self.diversity_weight = weights.get('diversity', 0.2)  # å¤šæ ·æ€§æƒé‡
        
        # Tokené™åˆ¶é…ç½®
        self.max_tokens = self.scheduler_config.get('max_tokens', 8000)
        self.min_candidates = self.scheduler_config.get('min_candidates', 3)
        self.max_candidates = self.scheduler_config.get('max_candidates', 10)
        
        # å¤šæ ·æ€§é…ç½®
        self.diversity_threshold = self.scheduler_config.get('diversity_threshold', 0.85)
        self.min_diversity_score = self.scheduler_config.get('min_diversity_score', 0.3)
        
        # ç»“æ„æƒé‡é…ç½®
        self.pagerank_bonus = self.scheduler_config.get('pagerank_bonus', 0.2)
        self.multi_source_bonus = self.scheduler_config.get('multi_source_bonus', 0.1)
        self.graph_entity_bonus = self.scheduler_config.get('graph_entity_bonus', 0.15)
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ç”¨äºå‘é‡è®¡ç®—
        self.llm_client = LLMClient(config)
        
        print(f"ğŸ“‹ ä¼˜å…ˆçº§è°ƒåº¦å™¨åˆå§‹åŒ–: å¯ç”¨={self.enabled}, æƒé‡=[ç›¸å…³åº¦:{self.relevance_weight}, ç»“æ„:{self.structure_weight}, å¤šæ ·æ€§:{self.diversity_weight}]")
    
    def schedule_candidates(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¯¹å€™é€‰ç‰‡æ®µè¿›è¡Œä¼˜å…ˆçº§æ’åºä¸ç­›é€‰
        
        Args:
            candidates: å€™é€‰ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            æœ€ç»ˆç”¨äºä¸Šä¸‹æ–‡æ‹¼æ¥çš„ç‰‡æ®µé›†åˆ
        """
        if not self.enabled:
            # å¦‚æœæœªå¯ç”¨è°ƒåº¦å™¨ï¼Œè¿”å›åŸæœ‰é€»è¾‘ï¼ˆå‰5ä¸ªï¼‰
            return candidates[:5]
        
        if not candidates:
            return []
        
        print(f"\nğŸ“‹ [ä¼˜å…ˆçº§è°ƒåº¦] å¼€å§‹å¤„ç† {len(candidates)} ä¸ªå€™é€‰ç‰‡æ®µ")
        
        # 1. è®¡ç®—æ¯ä¸ªå€™é€‰ç‰‡æ®µçš„ä¼˜å…ˆçº§åˆ†æ•°
        scored_candidates = self._compute_priority_scores(candidates)
        
        # 2. åŸºäºä¼˜å…ˆçº§å’Œå¤šæ ·æ€§é€‰æ‹©æœ€ä¼˜ç»„åˆ
        selected_candidates = self._select_optimal_combination(scored_candidates)
        
        # 3. éªŒè¯tokené™åˆ¶
        final_candidates = self._enforce_token_limit(selected_candidates)
        
        print(f"ğŸ“‹ [ä¼˜å…ˆçº§è°ƒåº¦] æœ€ç»ˆé€‰æ‹© {len(final_candidates)} ä¸ªç‰‡æ®µ")
        
        return final_candidates
    
    def _compute_priority_scores(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        è®¡ç®—æ¯ä¸ªå€™é€‰ç‰‡æ®µçš„ä¼˜å…ˆçº§åˆ†æ•°
        
        Args:
            candidates: å€™é€‰ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            åŒ…å«ä¼˜å…ˆçº§åˆ†æ•°çš„å€™é€‰ç‰‡æ®µåˆ—è¡¨
        """
        scored_candidates = []
        
        for candidate in candidates:
            # 1. æ£€ç´¢ç›¸å…³åº¦åˆ†æ•°
            relevance_score = self._compute_relevance_score(candidate)
            
            # 2. ç»“æ„æƒé‡åˆ†æ•°
            structure_score = self._compute_structure_score(candidate)
            
            # 3. è®¡ç®—ç»¼åˆä¼˜å…ˆçº§åˆ†æ•°ï¼ˆæš‚ä¸è€ƒè™‘å¤šæ ·æ€§ï¼Œåç»­åœ¨é€‰æ‹©é˜¶æ®µå¤„ç†ï¼‰
            priority_score = (
                self.relevance_weight * relevance_score +
                self.structure_weight * structure_score
            )
            
            # æ·»åŠ åˆ†æ•°ä¿¡æ¯åˆ°å€™é€‰ç‰‡æ®µ
            enhanced_candidate = candidate.copy()
            enhanced_candidate.update({
                'relevance_score': relevance_score,
                'structure_score': structure_score,
                'priority_score': priority_score
            })
            
            scored_candidates.append(enhanced_candidate)
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†æ•°æ’åº
        scored_candidates.sort(key=lambda x: x['priority_score'], reverse=True)
        
        return scored_candidates
    
    def _compute_relevance_score(self, candidate: Dict[str, Any]) -> float:
        """
        è®¡ç®—æ£€ç´¢ç›¸å…³åº¦åˆ†æ•°
        
        Args:
            candidate: å€™é€‰ç‰‡æ®µ
            
        Returns:
            ç›¸å…³åº¦åˆ†æ•° (0-1)
        """
        # ä¼˜å…ˆä½¿ç”¨å½’ä¸€åŒ–ç›¸ä¼¼åº¦ï¼Œå…¶æ¬¡ä½¿ç”¨åŸå§‹ç›¸ä¼¼åº¦
        if 'normalized_similarity' in candidate:
            base_score = candidate['normalized_similarity']
        elif 'similarity' in candidate:
            base_score = candidate['similarity']
        else:
            base_score = 0.5  # é»˜è®¤ä¸­ç­‰ç›¸å…³åº¦
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
        base_score = max(0.0, min(1.0, float(base_score)))
        
        # å¦‚æœæœ‰LLMé‡æ’åºåˆ†æ•°ï¼Œç»™äºˆé¢å¤–åŠ æƒ
        if candidate.get('source') == 'llm_rerank' or 'rerank_score' in candidate:
            rerank_bonus = 0.1
            base_score = min(1.0, base_score + rerank_bonus)
        
        return base_score
    
    def _compute_structure_score(self, candidate: Dict[str, Any]) -> float:
        """
        è®¡ç®—ç»“æ„æƒé‡åˆ†æ•°
        
        Args:
            candidate: å€™é€‰ç‰‡æ®µ
            
        Returns:
            ç»“æ„æƒé‡åˆ†æ•° (0-1)
        """
        structure_score = 0.0
        
        # 1. PageRankå®ä½“åŠ æƒ
        if self._has_high_pagerank_entities(candidate):
            structure_score += self.pagerank_bonus
        
        # 2. å¤šæºæ£€ç´¢å‘½ä¸­åŠ æƒ
        if self._is_multi_source_hit(candidate):
            structure_score += self.multi_source_bonus
        
        # 3. å›¾è°±å®ä½“ç›¸å…³æ€§åŠ æƒ
        if self._has_graph_entities(candidate):
            structure_score += self.graph_entity_bonus
        
        # 4. æ£€ç´¢ç±»å‹åŠ æƒ
        retrieval_type_bonus = self._get_retrieval_type_bonus(candidate)
        structure_score += retrieval_type_bonus
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
        return max(0.0, min(1.0, structure_score))
    
    def _has_high_pagerank_entities(self, candidate: Dict[str, Any]) -> bool:
        """
        æ£€æŸ¥å€™é€‰ç‰‡æ®µæ˜¯å¦åŒ…å«é«˜PageRankå®ä½“
        
        Args:
            candidate: å€™é€‰ç‰‡æ®µ
            
        Returns:
            æ˜¯å¦åŒ…å«é«˜PageRankå®ä½“
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰PageRankç›¸å…³ä¿¡æ¯
        if 'pagerank_score' in candidate:
            return candidate['pagerank_score'] > 0.1
        
        # æ£€æŸ¥æ£€ç´¢ç±»å‹æ˜¯å¦ä¸ºå›¾è°±ç›¸å…³
        retrieval_types = candidate.get('retrieval_types', [])
        if isinstance(retrieval_types, list):
            return any('graph' in rt.lower() or 'entity' in rt.lower() for rt in retrieval_types)
        
        retrieval_type = candidate.get('retrieval_type', '')
        return 'graph' in retrieval_type.lower() or 'entity' in retrieval_type.lower()
    
    def _is_multi_source_hit(self, candidate: Dict[str, Any]) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæºæ£€ç´¢å‘½ä¸­
        
        Args:
            candidate: å€™é€‰ç‰‡æ®µ
            
        Returns:
            æ˜¯å¦ä¸ºå¤šæºæ£€ç´¢å‘½ä¸­
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªæ£€ç´¢ç±»å‹
        retrieval_types = candidate.get('retrieval_types', [])
        if isinstance(retrieval_types, list) and len(retrieval_types) > 1:
            return True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šæºæ ‡è®°
        return candidate.get('multi_source', False)
    
    def _has_graph_entities(self, candidate: Dict[str, Any]) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾è°±å®ä½“
        
        Args:
            candidate: å€™é€‰ç‰‡æ®µ
            
        Returns:
            æ˜¯å¦åŒ…å«å›¾è°±å®ä½“
        """
        # æ£€æŸ¥æ£€ç´¢ç±»å‹
        retrieval_type = candidate.get('retrieval_type', '')
        retrieval_types = candidate.get('retrieval_types', [])
        
        if isinstance(retrieval_types, list):
            graph_types = ['graph', 'entity', 'enhanced_graph']
            return any(any(gt in rt.lower() for gt in graph_types) for rt in retrieval_types)
        
        graph_keywords = ['graph', 'entity', 'enhanced_graph']
        return any(keyword in retrieval_type.lower() for keyword in graph_keywords)
    
    def _get_retrieval_type_bonus(self, candidate: Dict[str, Any]) -> float:
        """
        æ ¹æ®æ£€ç´¢ç±»å‹ç»™äºˆåŠ æƒ
        
        Args:
            candidate: å€™é€‰ç‰‡æ®µ
            
        Returns:
            æ£€ç´¢ç±»å‹åŠ æƒåˆ†æ•°
        """
        retrieval_type = candidate.get('retrieval_type', '').lower()
        retrieval_types = candidate.get('retrieval_types', [])
        
        # æ£€ç´¢ç±»å‹æƒé‡æ˜ å°„
        type_weights = {
            'enhanced_graph': 0.15,
            'graph': 0.1,
            'entity': 0.1,
            'vector': 0.05,
            'bm25': 0.05,
            'llm_rerank': 0.1
        }
        
        max_bonus = 0.0
        
        # æ£€æŸ¥å•ä¸€æ£€ç´¢ç±»å‹
        for type_name, weight in type_weights.items():
            if type_name in retrieval_type:
                max_bonus = max(max_bonus, weight)
        
        # æ£€æŸ¥å¤šæ£€ç´¢ç±»å‹
        if isinstance(retrieval_types, list):
            for rt in retrieval_types:
                rt_lower = rt.lower()
                for type_name, weight in type_weights.items():
                    if type_name in rt_lower:
                        max_bonus = max(max_bonus, weight)
        
        return max_bonus
    
    def _select_optimal_combination(self, scored_candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        åŸºäºä¼˜å…ˆçº§å’Œå¤šæ ·æ€§é€‰æ‹©æœ€ä¼˜ç»„åˆ
        
        Args:
            scored_candidates: å·²è¯„åˆ†çš„å€™é€‰ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            é€‰æ‹©çš„æœ€ä¼˜ç‰‡æ®µç»„åˆ
        """
        if not scored_candidates:
            return []
        
        selected = []
        remaining = scored_candidates.copy()
        
        # é¦–å…ˆé€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„ç‰‡æ®µ
        selected.append(remaining.pop(0))
        
        # é€æ­¥æ·»åŠ ç‰‡æ®µï¼Œè€ƒè™‘å¤šæ ·æ€§
        while remaining and len(selected) < self.max_candidates:
            best_candidate = None
            best_score = -1
            best_idx = -1
            
            for idx, candidate in enumerate(remaining):
                # è®¡ç®—ä¸å·²é€‰ç‰‡æ®µçš„å¤šæ ·æ€§åˆ†æ•°
                diversity_score = self._compute_diversity_score(candidate, selected)
                
                # ç»¼åˆä¼˜å…ˆçº§å’Œå¤šæ ·æ€§è®¡ç®—æœ€ç»ˆåˆ†æ•°
                final_score = (
                    (self.relevance_weight + self.structure_weight) * candidate['priority_score'] +
                    self.diversity_weight * diversity_score
                )
                
                if final_score > best_score:
                    best_score = final_score
                    best_candidate = candidate
                    best_idx = idx
            
            # æ·»åŠ æœ€ä½³å€™é€‰ç‰‡æ®µ
            if best_candidate and best_score > self.min_diversity_score:
                selected.append(best_candidate)
                remaining.pop(best_idx)
            else:
                break
        
        # ç¡®ä¿è‡³å°‘æœ‰æœ€å°æ•°é‡çš„å€™é€‰ç‰‡æ®µ
        while len(selected) < self.min_candidates and remaining:
            selected.append(remaining.pop(0))
        
        return selected
    
    def _compute_diversity_score(self, candidate: Dict[str, Any], selected: List[Dict[str, Any]]) -> float:
        """
        è®¡ç®—å€™é€‰ç‰‡æ®µä¸å·²é€‰ç‰‡æ®µçš„å¤šæ ·æ€§åˆ†æ•°
        
        Args:
            candidate: å€™é€‰ç‰‡æ®µ
            selected: å·²é€‰æ‹©çš„ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            å¤šæ ·æ€§åˆ†æ•° (0-1ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šå¤šæ ·)
        """
        if not selected:
            return 1.0
        
        candidate_text = candidate.get('text', '')
        if not candidate_text:
            return 0.5
        
        # è®¡ç®—ä¸æ‰€æœ‰å·²é€‰ç‰‡æ®µçš„ç›¸ä¼¼åº¦
        similarities = []
        for selected_item in selected:
            selected_text = selected_item.get('text', '')
            if selected_text:
                similarity = self._compute_text_similarity(candidate_text, selected_text)
                similarities.append(similarity)
        
        if not similarities:
            return 1.0
        
        # ä½¿ç”¨æœ€å¤§ç›¸ä¼¼åº¦æ¥è®¡ç®—å¤šæ ·æ€§ï¼ˆæœ€å¤§ç›¸ä¼¼åº¦è¶Šä½ï¼Œå¤šæ ·æ€§è¶Šé«˜ï¼‰
        max_similarity = max(similarities)
        diversity_score = 1.0 - max_similarity
        
        # å¦‚æœç›¸ä¼¼åº¦è¿‡é«˜ï¼Œç»™äºˆæƒ©ç½š
        if max_similarity > self.diversity_threshold:
            diversity_score *= 0.5
        
        return max(0.0, min(1.0, diversity_score))
    
    def _compute_text_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        try:
            # ä½¿ç”¨ç®€å•çš„è¯æ±‡é‡å è®¡ç®—ç›¸ä¼¼åº¦
            tokens1 = set(improved_tokenize(text1))
            tokens2 = set(improved_tokenize(text2))
            
            if not tokens1 or not tokens2:
                return 0.0
            
            # è®¡ç®—Jaccardç›¸ä¼¼åº¦
            intersection = len(tokens1.intersection(tokens2))
            union = len(tokens1.union(tokens2))
            
            return safe_divide(intersection, union, 0.0)
            
        except Exception as e:
            print(f"âš ï¸ æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _enforce_token_limit(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ ¹æ®tokené™åˆ¶ç­›é€‰å€™é€‰ç‰‡æ®µ
        
        Args:
            candidates: å€™é€‰ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            ç¬¦åˆtokené™åˆ¶çš„ç‰‡æ®µåˆ—è¡¨
        """
        if not candidates:
            return []
        
        selected = []
        total_tokens = 0
        
        for candidate in candidates:
            # ä¼°ç®—æ–‡æœ¬tokenæ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦æ•° + è‹±æ–‡å•è¯æ•°ï¼‰
            text = candidate.get('text', '')
            estimated_tokens = self._estimate_tokens(text)
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if total_tokens + estimated_tokens <= self.max_tokens:
                selected.append(candidate)
                total_tokens += estimated_tokens
            else:
                # å¦‚æœæ·»åŠ å½“å‰ç‰‡æ®µä¼šè¶…è¿‡é™åˆ¶ï¼Œæ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰æœ€å°æ•°é‡
                if len(selected) >= self.min_candidates:
                    break
                else:
                    # å¦‚æœè¿˜æ²¡è¾¾åˆ°æœ€å°æ•°é‡ï¼Œå°è¯•æˆªæ–­å½“å‰ç‰‡æ®µ
                    remaining_tokens = self.max_tokens - total_tokens
                    if remaining_tokens > 100:  # è‡³å°‘ä¿ç•™100ä¸ªtoken
                        truncated_candidate = self._truncate_candidate(candidate, remaining_tokens)
                        selected.append(truncated_candidate)
                        break
        
        print(f"ğŸ“‹ [Tokené™åˆ¶] æ€»è®¡çº¦ {total_tokens} tokensï¼Œé€‰æ‹© {len(selected)} ä¸ªç‰‡æ®µ")
        
        return selected
    
    def _estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            ä¼°ç®—çš„tokenæ•°é‡
        """
        if not text:
            return 0
        
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦æŒ‰1.5ä¸ªtokenè®¡ç®—ï¼Œè‹±æ–‡å•è¯æŒ‰1ä¸ªtokenè®¡ç®—
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        english_words = len(text.split()) - chinese_chars  # ç²—ç•¥ä¼°ç®—è‹±æ–‡å•è¯æ•°
        
        estimated_tokens = int(chinese_chars * 1.5 + english_words)
        return max(estimated_tokens, len(text) // 4)  # æœ€å°‘æŒ‰4å­—ç¬¦1tokenè®¡ç®—
    
    def _truncate_candidate(self, candidate: Dict[str, Any], max_tokens: int) -> Dict[str, Any]:
        """
        æˆªæ–­å€™é€‰ç‰‡æ®µä»¥ç¬¦åˆtokené™åˆ¶
        
        Args:
            candidate: å€™é€‰ç‰‡æ®µ
            max_tokens: æœ€å¤§tokenæ•°é‡
            
        Returns:
            æˆªæ–­åçš„å€™é€‰ç‰‡æ®µ
        """
        text = candidate.get('text', '')
        if not text:
            return candidate
        
        # ç®€å•æˆªæ–­ï¼šæŒ‰å­—ç¬¦æ•°ä¼°ç®—
        max_chars = max_tokens * 3  # ç²—ç•¥ä¼°ç®—
        if len(text) <= max_chars:
            return candidate
        
        # æˆªæ–­æ–‡æœ¬ï¼Œå°½é‡åœ¨å¥å·å¤„æˆªæ–­
        truncated_text = text[:max_chars]
        last_period = truncated_text.rfind('ã€‚')
        if last_period > max_chars * 0.7:  # å¦‚æœå¥å·ä½ç½®åˆç†
            truncated_text = truncated_text[:last_period + 1]
        else:
            truncated_text += '...'
        
        # åˆ›å»ºæˆªæ–­åçš„å€™é€‰ç‰‡æ®µ
        truncated_candidate = candidate.copy()
        truncated_candidate['text'] = truncated_text
        truncated_candidate['truncated'] = True
        
        return truncated_candidate