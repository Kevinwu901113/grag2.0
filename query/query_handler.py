import json
import os
import faiss
import numpy as np
from query.optimized_theme_matcher import ThemeMatcher
from query.reranker import SimpleReranker, LLMReranker
from query.enhanced_retriever import EnhancedRetriever
from query.query_rewriter import QueryRewriter, is_query_rewrite_enabled
from query.context_scheduler import PriorityContextScheduler
from llm.llm import LLMClient
from llm.answer_selector import AnswerSelector
from graph.graph_utils import extract_entity_names, match_entities_in_query, extract_subgraph, summarize_subgraph
from query.query_classifier import classify_query_lightweight
from utils.io import load_chunks, load_vector_index, load_graph

def direct_vector_search(query: str, index, id_map: dict, chunks: list, config: dict, top_k: int = 5) -> list:
    """
    ç›´æ¥ä½¿ç”¨Faisså‘é‡ç´¢å¼•è¿›è¡Œç²¾ç¡®æŸ¥è¯¢ï¼Œè·³è¿‡ä¸»é¢˜æ‘˜è¦
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        index: Faissç´¢å¼•
        id_map: IDæ˜ å°„å­—å…¸
        chunks: æ–‡æ¡£å—åˆ—è¡¨
        config: é…ç½®å­—å…¸
        top_k: è¿”å›çš„æœ€å¤§ç»“æœæ•°é‡
        
    Returns:
        æ£€ç´¢åˆ°çš„å€™é€‰ç»“æœåˆ—è¡¨ï¼ˆåŒ…å«textå’Œsimilarityå­—æ®µï¼‰
    """
    try:
        # ç»Ÿä¸€ä½¿ç”¨LLMClientè¿›è¡ŒåµŒå…¥å‘é‡ç”Ÿæˆ
        llm_client = LLMClient(config)
        
        # ç¼–ç æŸ¥è¯¢ï¼ˆå¯ç”¨æ–‡æœ¬é¢„å¤„ç†å’Œç»´åº¦éªŒè¯ï¼‰
        query_embeddings = llm_client.embed([query], normalize_text=True, validate_dim=True)
        if not query_embeddings:
            print(f"æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥: {query}")
            return []
            
        query_emb = np.array([query_embeddings[0]]).astype('float32')
        
        # æ£€æŸ¥å‘é‡æœ‰æ•ˆæ€§
        if query_emb.shape[1] == 0:
            print(f"æŸ¥è¯¢å‘é‡ä¸ºç©º: {query}")
            return []
            
        # æ£€æŸ¥æŸ¥è¯¢å‘é‡å¹…åº¦ï¼ˆå¼‚å¸¸æ£€æµ‹ï¼‰
        vector_magnitude = np.linalg.norm(query_emb)
        if vector_magnitude < 1e-6:
            print(f"âš ï¸ æŸ¥è¯¢å‘é‡å¹…åº¦è¿‡å°: {vector_magnitude}, å¯èƒ½å­˜åœ¨å¼‚å¸¸")
            return []
        
        # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡ä»¥ç¡®ä¿è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        import faiss
        faiss.normalize_L2(query_emb)
        
        # æ£€æŸ¥å½’ä¸€åŒ–åçš„å‘é‡
        if np.any(np.isnan(query_emb)) or np.any(np.isinf(query_emb)):
            print(f"æŸ¥è¯¢å‘é‡åŒ…å«æ— æ•ˆå€¼: {query}")
            return []
        
        # æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡
        scores, indices = index.search(query_emb, top_k)
        
        candidates = []
        print("\n[ç›´æ¥å‘é‡æ£€ç´¢è¯¦æƒ…]")
        print("-" * 50)
        
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx == -1:  # Faissè¿”å›-1è¡¨ç¤ºæ— æ•ˆç»“æœ
                continue
                
            # ä»IDæ˜ å°„ä¸­è·å–åŸå§‹chunk ID
            chunk_id = id_map.get(str(idx))
            if chunk_id is None:
                continue
                
            # æŸ¥æ‰¾å¯¹åº”çš„æ–‡æ¡£å—
            chunk_text = next((c['text'] for c in chunks if c['id'] == chunk_id), None)
            if chunk_text:
                candidate = {
                    'text': chunk_text,
                    'similarity': float(score),
                    'id': chunk_id,
                    'source': 'direct_vector'
                }
                candidates.append(candidate)
                
                print(f"æ–‡æ¡£å— #{i+1}:")
                print(f"  ID: {chunk_id}")
                print(f"  ç›¸ä¼¼åº¦: {score:.4f}")
                print(f"  å†…å®¹: {chunk_text[:100]}..." if len(chunk_text) > 100 else f"  å†…å®¹: {chunk_text}")
                print("-" * 50)
        
        return candidates
        
    except Exception as e:
        print(f"âŒ ç›´æ¥å‘é‡æœç´¢å¤±è´¥: {e}")
        return []

def handle_query(query: str, config: dict, work_dir: str, mode: str = "auto", precise: bool = False, use_reranker: str = "simple", use_enhanced_retrieval: bool = True) -> dict:
    """
    å¤„ç†å•ä¸ªæŸ¥è¯¢
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        config: é…ç½®å­—å…¸
        work_dir: å·¥ä½œç›®å½•
        mode: æŸ¥è¯¢æ¨¡å¼
        precise: æ˜¯å¦ç²¾ç¡®æŸ¥è¯¢
        use_reranker: é‡æ’åºå™¨ç±»å‹
        use_enhanced_retrieval: æ˜¯å¦ä½¿ç”¨å¢å¼ºæ£€ç´¢
        
    Returns:
        åŒ…å«ç­”æ¡ˆã€æ¥æºå’Œå¤„ç†æ—¶é—´çš„å­—å…¸
    """
    import time
    start_time = time.time()
    timing_info = {}  # å­˜å‚¨å„ç¯èŠ‚çš„è®¡æ—¶ä¿¡æ¯
    
    # 1. æŸ¥è¯¢æ”¹å†™å¤„ç†
    rewrite_start = time.time()
    original_query = query
    rewrite_result = None
    
    if is_query_rewrite_enabled(config):
        try:
            rewriter = QueryRewriter(config)
            rewrite_result = rewriter.rewrite_query(query)
            
            # æ ¹æ®è¯„ä¼°ç»“æœå†³å®šæ˜¯å¦ä½¿ç”¨æ”¹å†™åçš„æŸ¥è¯¢
            if (rewrite_result.get("evaluation", {}).get("recommendation") == "accept" or 
                not rewrite_result.get("evaluation")):
                query = rewrite_result["rewritten_query"]
                print(f"[æŸ¥è¯¢æ”¹å†™] åŸå§‹æŸ¥è¯¢: {original_query}")
                print(f"[æŸ¥è¯¢æ”¹å†™] æ”¹å†™æŸ¥è¯¢: {query}")
                print(f"[æŸ¥è¯¢æ”¹å†™] ç­–ç•¥: {rewrite_result['strategy']}")
            else:
                print(f"[æŸ¥è¯¢æ”¹å†™] æ”¹å†™è¢«æ‹’ç»ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
                query = original_query
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢æ”¹å†™å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢: {e}")
            query = original_query
    
    timing_info['query_rewrite'] = time.time() - rewrite_start
    print(f"â±ï¸ æŸ¥è¯¢æ”¹å†™è€—æ—¶: {timing_info['query_rewrite']:.3f}ç§’")
    
    # 2. æ•°æ®åŠ è½½
    load_start = time.time()
    llm = LLMClient(config)
    chunks = load_chunks(work_dir)
    index, id_map = load_vector_index(work_dir)
    
    # åŠ è½½å›¾è°±
    graph_path = os.path.join(work_dir, "graph.json")
    graph = load_graph(work_dir) if os.path.exists(graph_path) else None
    entity_names = extract_entity_names(graph) if graph else set()
    
    timing_info['data_loading'] = time.time() - load_start
    print(f"â±ï¸ æ•°æ®åŠ è½½è€—æ—¶: {timing_info['data_loading']:.3f}ç§’")
    
    # 3. æŸ¥è¯¢åˆ†ç±»
    classification_start = time.time()
    if mode == "auto":
        original_mode, original_precise = classify_query_lightweight(query, config)
        from query.query_enhancer import enhance_query_classification
        mode, precise = enhance_query_classification(query, original_mode, original_precise)
    
    timing_info['query_classification'] = time.time() - classification_start
    print(f"â±ï¸ æŸ¥è¯¢åˆ†ç±»è€—æ—¶: {timing_info['query_classification']:.3f}ç§’")
    
    # å¤„ç†noragæ¨¡å¼
    if mode == "norag":
        norag_start = time.time()
        response = llm.generate(f"è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n{query}")
        timing_info['norag_generation'] = time.time() - norag_start
        print(f"â±ï¸ NoRAGç”Ÿæˆè€—æ—¶: {timing_info['norag_generation']:.3f}ç§’")
        return {
            'answer': response.strip(),
            'sources': [],
            'processing_time': time.time() - start_time,
            'enhanced_retrieval': False,
            'timing_info': timing_info
        }
    
    # 4. æ–‡æ¡£æ£€ç´¢
    retrieval_start = time.time()
    candidates = []
    
    if use_enhanced_retrieval:
        # ä½¿ç”¨å¢å¼ºæ£€ç´¢å™¨
        retriever = EnhancedRetriever(config, work_dir)
        candidates = retriever.retrieve(query, top_k=10)
    else:
        # ä¼ ç»Ÿæ£€ç´¢æ–¹æ³•
        if precise and index is not None:
            candidates = direct_vector_search(query, index, id_map, chunks, config, top_k=10)
        else:
            matcher = ThemeMatcher(chunks, config)
            matches = matcher.match(query, top_k=10, min_score=0.3)
            
            for match in matches:
                topic_id = match["node_id"]
                similarity = match.get("similarity", 0.0)
                topic_title = match.get("title", "æ— æ ‡é¢˜")
                match_text = next((c['text'] for c in chunks if c['id'] == topic_id), None)
                
                if match_text:
                    candidate = {
                        'text': match_text,
                        'similarity': similarity,
                        'id': topic_id,
                        'title': topic_title,
                        'source': 'theme_matcher'
                    }
                    candidates.append(candidate)
    
    timing_info['document_retrieval'] = time.time() - retrieval_start
    print(f"â±ï¸ æ–‡æ¡£æ£€ç´¢è€—æ—¶: {timing_info['document_retrieval']:.3f}ç§’")
    
    # 5. é‡æ’åº
    rerank_start = time.time()
    if use_reranker != "none" and candidates:
        rerank_config = config.get("rerank", {})
        if use_reranker == "llm":
            reranker = LLMReranker(llm, rerank_config)
        else:
            reranker = SimpleReranker(rerank_config)
        
        candidates = reranker.rerank(query, candidates, top_k=10)  # å¢åŠ é‡æ’åºæ•°é‡ä¾›è°ƒåº¦å™¨é€‰æ‹©
    
    timing_info['reranking'] = time.time() - rerank_start
    print(f"â±ï¸ é‡æ’åºè€—æ—¶: {timing_info['reranking']:.3f}ç§’")
    
    # 6. ä¸Šä¸‹æ–‡è°ƒåº¦
    scheduling_start = time.time()
    context_scheduler = PriorityContextScheduler(config)
    candidates = context_scheduler.schedule_candidates(candidates)
    
    timing_info['context_scheduling'] = time.time() - scheduling_start
    print(f"â±ï¸ ä¸Šä¸‹æ–‡è°ƒåº¦è€—æ—¶: {timing_info['context_scheduling']:.3f}ç§’")
    
    # 7. æ–‡æœ¬æå–å’Œå›¾è°±å¤„ç†
    graph_start = time.time()
    # æå–æ–‡æœ¬ç”¨äºç”Ÿæˆå›ç­”
    retrieved = [c['text'] for c in candidates]
    context = "\n".join(retrieved) if retrieved else "ï¼ˆæœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æœ¬å†…å®¹ï¼‰"
    
    # å›¾è°±æ‘˜è¦
    graph_summary = ""
    if graph:
        try:
            matched_entities = match_entities_in_query(query, entity_names)
            subgraph = extract_subgraph(graph, matched_entities)
            graph_summary = summarize_subgraph(subgraph)
        except Exception:
            pass
    
    timing_info['graph_processing'] = time.time() - graph_start
    print(f"â±ï¸ å›¾è°±å¤„ç†è€—æ—¶: {timing_info['graph_processing']:.3f}ç§’")
    
    # 8. ç­”æ¡ˆç”Ÿæˆ
    generation_start = time.time()
    answer_selector_config = config.get('answer_selector', {})
    answer_selector = AnswerSelector(llm, answer_selector_config)
    
    # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
    full_context = context
    if graph_summary:
        full_context += f"\n\nå›¾è°±æ‘˜è¦ï¼š\n{graph_summary}"
    
    # æå–å®ä½“ç”¨äºå¤æ‚åº¦åˆ¤æ–­
    entities = []
    if graph:
        try:
            entities = match_entities_in_query(query, entity_names)
        except Exception:
            pass
    
    # ä½¿ç”¨ç­”æ¡ˆé€‰æ‹©å™¨ç”Ÿæˆæœ€ä½³ç­”æ¡ˆ
    answer, selection_metadata = answer_selector.select_best_answer(
        query=query, 
        context=full_context, 
        entities=entities
    )
    
    timing_info['answer_generation'] = time.time() - generation_start
    print(f"â±ï¸ ç­”æ¡ˆç”Ÿæˆè€—æ—¶: {timing_info['answer_generation']:.3f}ç§’")
    
    # è®¡ç®—æ€»å¤„ç†æ—¶é—´
    total_time = time.time() - start_time
    timing_info['total_processing'] = total_time
    
    # è¾“å‡ºè¯¦ç»†è®¡æ—¶ç»Ÿè®¡
    print(f"\nğŸ“Š è¯¦ç»†è®¡æ—¶ç»Ÿè®¡:")
    print(f"  æŸ¥è¯¢æ”¹å†™: {timing_info.get('query_rewrite', 0):.3f}ç§’")
    print(f"  æ•°æ®åŠ è½½: {timing_info.get('data_loading', 0):.3f}ç§’")
    print(f"  æŸ¥è¯¢åˆ†ç±»: {timing_info.get('query_classification', 0):.3f}ç§’")
    print(f"  æ–‡æ¡£æ£€ç´¢: {timing_info.get('document_retrieval', 0):.3f}ç§’")
    print(f"  é‡æ’åº: {timing_info.get('reranking', 0):.3f}ç§’")
    print(f"  ä¸Šä¸‹æ–‡è°ƒåº¦: {timing_info.get('context_scheduling', 0):.3f}ç§’")
    print(f"  å›¾è°±å¤„ç†: {timing_info.get('graph_processing', 0):.3f}ç§’")
    print(f"  ç­”æ¡ˆç”Ÿæˆ: {timing_info.get('answer_generation', 0):.3f}ç§’")
    print(f"  æ€»è®¡: {total_time:.3f}ç§’")
    
    # æ„å»ºè¿”å›ç»“æœ
    result = {
        'answer': answer.strip(),
        'sources': candidates,
        'processing_time': total_time,
        'enhanced_retrieval': use_enhanced_retrieval,
        'answer_selection': selection_metadata,
        'query_rewrite': {
            'enabled': is_query_rewrite_enabled(config),
            'original_query': original_query,
            'final_query': query,
            'rewrite_result': rewrite_result
        },
        'timing_info': timing_info
    }
    
    # ä¿å­˜æŸ¥è¯¢è®°å½•åˆ°query.jsonæ–‡ä»¶
    try:
        # æ„å»ºè¯¦ç»†çš„å¬å›æ–‡æ¡£ä¿¡æ¯
        sources_detail = []
        for i, source in enumerate(candidates, 1):
            source_info = {
                'rank': i,
                'id': source.get('id', 'unknown'),
                'similarity': source.get('similarity', 0),
                'text': source['text'],
                'title': source.get('title', ''),
                'source_type': source.get('source', 'unknown')
            }
            
            # æ·»åŠ æ£€ç´¢ç±»å‹ä¿¡æ¯
            if 'retrieval_types' in source:
                source_info['retrieval_types'] = source['retrieval_types']
            elif 'retrieval_type' in source:
                source_info['retrieval_type'] = source['retrieval_type']
            
            sources_detail.append(source_info)
        
        query_record = {
            'query': {
                'original': original_query,
                'final': query,
                'mode': mode,
                'precise': precise,
                'use_reranker': use_reranker,
                'use_enhanced_retrieval': use_enhanced_retrieval
            },
            'result': {
                'answer': result['answer'],
                'sources_count': len(result['sources']),
                'processing_time': result['processing_time'],
                'enhanced_retrieval': result['enhanced_retrieval']
            },
            'sources': sources_detail,
            'metadata': {
                'answer_selection': result['answer_selection'],
                'query_rewrite': result['query_rewrite']
            },
            'timestamp': time.time()
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        query_json_path = os.path.join(work_dir, 'query.json')
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯»å–ç°æœ‰å†…å®¹å¹¶è¿½åŠ æ–°è®°å½•
        if os.path.exists(query_json_path):
            try:
                with open(query_json_path, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                if not isinstance(existing_data, list):
                    existing_data = [existing_data]  # å…¼å®¹æ—§æ ¼å¼
            except (json.JSONDecodeError, Exception):
                existing_data = []  # å¦‚æœæ–‡ä»¶æŸåï¼Œé‡æ–°å¼€å§‹
        else:
            existing_data = []
        
        # æ·»åŠ æ–°è®°å½•
        existing_data.append(query_record)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(query_json_path, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
            
        print(f"\nğŸ“„ æŸ¥è¯¢è®°å½•å·²ä¿å­˜åˆ°: {query_json_path}")
        
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æŸ¥è¯¢è®°å½•å¤±è´¥: {e}")
    
    return result

def run_query_loop(config: dict, work_dir: str, logger=None):
    """
    è¿è¡ŒæŸ¥è¯¢å¾ªç¯
    
    Args:
        config: é…ç½®å­—å…¸
        work_dir: å·¥ä½œç›®å½•
        logger: æ—¥å¿—å¯¹è±¡
    """
    if logger:
        logger.info("å¼€å§‹æŸ¥è¯¢å¾ªç¯")
    
    print("\nğŸ” RAGæŸ¥è¯¢ç³»ç»Ÿå·²å¯åŠ¨")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("è¾“å…¥ 'mode <æ¨¡å¼>' åˆ‡æ¢æŸ¥è¯¢æ¨¡å¼ (norag, hybrid_precise, hybrid_imprecise, auto)")
    print("è¾“å…¥ 'reranker <ç±»å‹>' åˆ‡æ¢é‡æ’åºå™¨ (simple, llm, none)")
    print("è¾“å…¥ 'enhanced <on/off>' åˆ‡æ¢å¢å¼ºæ£€ç´¢")
    print("-" * 50)
    
    current_mode = "auto"
    current_reranker = "simple"
    use_enhanced = True
    
    while True:
        try:
            user_input = input("\nè¯·è¾“å…¥æŸ¥è¯¢: ").strip()
            
            if user_input.lower() in ['quit', 'exit']:
                print("å†è§ï¼")
                break
            
            if user_input.startswith('mode '):
                new_mode = user_input[5:].strip()
                if new_mode in ['norag', 'hybrid_precise', 'hybrid_imprecise', 'auto']:
                    current_mode = new_mode
                    print(f"âœ… æŸ¥è¯¢æ¨¡å¼å·²åˆ‡æ¢ä¸º: {current_mode}")
                else:
                    print("âŒ æ— æ•ˆæ¨¡å¼ï¼Œå¯é€‰: norag, hybrid_precise, hybrid_imprecise, auto")
                continue
            
            if user_input.startswith('reranker '):
                new_reranker = user_input[9:].strip()
                if new_reranker in ['simple', 'llm', 'none']:
                    current_reranker = new_reranker
                    print(f"âœ… é‡æ’åºå™¨å·²åˆ‡æ¢ä¸º: {current_reranker}")
                else:
                    print("âŒ æ— æ•ˆé‡æ’åºå™¨ï¼Œå¯é€‰: simple, llm, none")
                continue
                
            if user_input.startswith('enhanced '):
                enhanced_setting = user_input[9:].strip().lower()
                if enhanced_setting in ['on', 'true', '1']:
                    use_enhanced = True
                    print("âœ… å¢å¼ºæ£€ç´¢å·²å¯ç”¨")
                elif enhanced_setting in ['off', 'false', '0']:
                    use_enhanced = False
                    print("âœ… å¢å¼ºæ£€ç´¢å·²ç¦ç”¨")
                else:
                    print("âŒ æ— æ•ˆè®¾ç½®ï¼Œè¯·ä½¿ç”¨ on/off")
                continue
            
            if not user_input:
                continue
            
            print(f"\nğŸ” æŸ¥è¯¢æ¨¡å¼: {current_mode}, é‡æ’åºå™¨: {current_reranker}, å¢å¼ºæ£€ç´¢: {'å¼€å¯' if use_enhanced else 'å…³é—­'}")
            
            # å¤„ç†æŸ¥è¯¢
            result = handle_query(
                query=user_input,
                config=config,
                work_dir=work_dir,
                mode=current_mode,
                use_reranker=current_reranker,
                use_enhanced_retrieval=use_enhanced
            )
            
            # æ˜¾ç¤ºç»“æœ
            print(f"\nğŸ“ å›ç­”:")
            print(result['answer'])
            
            # æ˜¾ç¤ºç­”æ¡ˆé€‰æ‹©ä¿¡æ¯
            if 'answer_selection' in result:
                selection_info = result['answer_selection']
                method = selection_info.get('method', 'unknown')
                candidates_count = selection_info.get('candidates_count', 0)
                
                if method == 'multi' and candidates_count > 1:
                    best_score = selection_info.get('best_score', 0)
                    print(f"\nğŸ¯ ç­”æ¡ˆé€‰æ‹©: å¤šå€™é€‰æ¨¡å¼ ({candidates_count}ä¸ªå€™é€‰ç­”æ¡ˆ, æœ€ä½³è¯„åˆ†: {best_score:.1f})")
                    if 'best_reasoning' in selection_info:
                        print(f"   é€‰æ‹©ç†ç”±: {selection_info['best_reasoning']}")
                elif method == 'multi':
                    print(f"\nğŸ¯ ç­”æ¡ˆé€‰æ‹©: å¤šå€™é€‰æ¨¡å¼ ({candidates_count}ä¸ªå€™é€‰ç­”æ¡ˆ)")
                else:
                    print(f"\nğŸ¯ ç­”æ¡ˆé€‰æ‹©: å•ä¸€ç­”æ¡ˆæ¨¡å¼")
            
            if result['sources']:
                print(f"\nğŸ“š å‚è€ƒæ¥æº ({len(result['sources'])}ä¸ª):")
                for i, source in enumerate(result['sources'], 1):
                    similarity = source.get('similarity', 0)
                    retrieval_info = ""
                    if 'retrieval_types' in source:
                        retrieval_info = f" [{','.join(source['retrieval_types'])}]"
                    elif 'retrieval_type' in source:
                        retrieval_info = f" [{source['retrieval_type']}]"
                    print(f"{i}. [ç›¸ä¼¼åº¦: {similarity:.3f}]{retrieval_info} {source['text']}")
            
            processing_time = result['processing_time']
            enhanced_status = "(å¢å¼ºæ£€ç´¢)" if result.get('enhanced_retrieval') else "(ä¼ ç»Ÿæ£€ç´¢)"
            print(f"\nâ±ï¸ æ€»å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’ {enhanced_status}")
            
            # æ˜¾ç¤ºè¯¦ç»†è®¡æ—¶ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if 'timing_info' in result:
                timing = result['timing_info']
                print(f"\nğŸ“Š å„ç¯èŠ‚è€—æ—¶è¯¦æƒ…:")
                if timing.get('query_rewrite', 0) > 0:
                    print(f"  æŸ¥è¯¢æ”¹å†™: {timing['query_rewrite']:.3f}ç§’")
                if timing.get('data_loading', 0) > 0:
                    print(f"  æ•°æ®åŠ è½½: {timing['data_loading']:.3f}ç§’")
                if timing.get('query_classification', 0) > 0:
                    print(f"  æŸ¥è¯¢åˆ†ç±»: {timing['query_classification']:.3f}ç§’")
                if timing.get('document_retrieval', 0) > 0:
                    print(f"  æ–‡æ¡£æ£€ç´¢: {timing['document_retrieval']:.3f}ç§’")
                if timing.get('reranking', 0) > 0:
                    print(f"  é‡æ’åº: {timing['reranking']:.3f}ç§’")
                if timing.get('context_scheduling', 0) > 0:
                    print(f"  ä¸Šä¸‹æ–‡è°ƒåº¦: {timing['context_scheduling']:.3f}ç§’")
                if timing.get('graph_processing', 0) > 0:
                    print(f"  å›¾è°±å¤„ç†: {timing['graph_processing']:.3f}ç§’")
                if timing.get('answer_generation', 0) > 0:
                    print(f"  ç­”æ¡ˆç”Ÿæˆ: {timing['answer_generation']:.3f}ç§’")
                if timing.get('norag_generation', 0) > 0:
                    print(f"  NoRAGç”Ÿæˆ: {timing['norag_generation']:.3f}ç§’")
            
        except KeyboardInterrupt:
            print("\n\nå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
