#!/usr/bin/env python3
import os
import json
import faiss
import numpy as np
from typing import List, Dict, Any, Tuple
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
from sentence_transformers import SentenceTransformer

from llm.llm import LLMClient
from query.reranker import SimpleReranker
from query.query_enhancer import get_query_enhancer
from graph.graph_utils import extract_entity_names, match_entities_in_query, extract_subgraph, summarize_subgraph, retrieve_by_entity
from graph.graph_subgraph_extractor import (
    extract_multi_hop_subgraph, 
    find_intermediate_entities, 
    generate_concise_graph_summary,
    get_multi_hop_retrieval_summary
)
from query.enhanced_graph_retriever import EnhancedGraphRetriever
from utils.io import load_chunks, load_vector_index, load_graph

class BM25Retriever:
    """
    ç®€åŒ–çš„BM25æ£€ç´¢å™¨å®ç°
    """
    
    def __init__(self, chunks: List[Dict[str, Any]], k1: float = 1.5, b: float = 0.75):
        self.chunks = chunks
        self.k1 = k1
        self.b = b
        # å¤„ç†ä¸åŒæ•°æ®æ ¼å¼çš„å…¼å®¹æ€§
        self.doc_texts = []
        for chunk in chunks:
            if 'text' in chunk:
                self.doc_texts.append(chunk['text'])
            elif 'sentences' in chunk:
                # å¤„ç†static_chunk_processorç”Ÿæˆçš„æ ¼å¼
                text = "\n".join(chunk['sentences']) if isinstance(chunk['sentences'], list) else str(chunk['sentences'])
                self.doc_texts.append(text)
            else:
                print(f"è­¦å‘Š: å— {chunk.get('id', 'unknown')} ç¼ºå°‘æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡å¤„ç†")
                continue
        self.doc_lengths = [len(list(jieba.cut(text))) for text in self.doc_texts]
        self.avg_doc_length = sum(self.doc_lengths) / len(self.doc_lengths) if self.doc_lengths else 0
        
        # æ„å»ºè¯é¢‘ç»Ÿè®¡
        self.term_freq = []
        self.doc_freq = defaultdict(int)
        
        # å¯¼å…¥æ”¹è¿›çš„åˆ†è¯å‡½æ•°
        from utils.common import improved_tokenize
        
        for text in self.doc_texts:
            terms = improved_tokenize(text)
            term_count = defaultdict(int)
            for term in terms:
                term_count[term] += 1
                
            self.term_freq.append(dict(term_count))
            
            # ç»Ÿè®¡æ–‡æ¡£é¢‘ç‡
            for term in set(terms):
                self.doc_freq[term] += 1
                
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        BM25æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        # ä½¿ç”¨æ”¹è¿›çš„åˆ†è¯å‡½æ•°
        from utils.common import improved_tokenize
        query_terms = improved_tokenize(query)
        scores = []
        
        for i, doc_tf in enumerate(self.term_freq):
            score = 0.0
            doc_len = self.doc_lengths[i]
            
            for term in query_terms:
                if term in doc_tf:
                    tf = doc_tf[term]
                    df = self.doc_freq[term]
                    # ä¿®å¤IDFè®¡ç®—ï¼Œé¿å…è´Ÿå€¼
                    idf = np.log((len(self.chunks) - df + 0.5) / (df + 0.5))
                    # å¦‚æœIDFä¸ºè´Ÿæ•°ï¼Œä½¿ç”¨æœ€å°æ­£å€¼
                    if idf <= 0:
                        idf = 0.01
                    
                    # BM25å…¬å¼
                    score += idf * (tf * (self.k1 + 1)) / (
                        tf + self.k1 * (1 - self.b + self.b * doc_len / self.avg_doc_length)
                    )
                    
            scores.append((score, i))
            
        # æ’åºå¹¶è¿”å›top_kç»“æœ
        scores.sort(reverse=True, key=lambda x: x[0])
        results = []
        
        for score, idx in scores[:top_k]:
            if score > 0:  # åªè¿”å›æœ‰åŒ¹é…çš„ç»“æœ
                result = self.chunks[idx].copy()
                result['similarity'] = float(score)
                result['retrieval_type'] = 'bm25'
                results.append(result)
                
        return results

class EnhancedRetriever:
    """
    å¢å¼ºæ£€ç´¢å™¨ï¼Œæ•´åˆå‘é‡æ£€ç´¢ã€BM25æ£€ç´¢å’ŒæŸ¥è¯¢æ‰©å±•
    """
    
    def __init__(self, config: Dict[str, Any], work_dir: str):
        self.config = config
        self.work_dir = work_dir
        self.llm_client = LLMClient(config)
        
        # åŠ è½½æ•°æ®å’Œç´¢å¼•
        self.chunks = self._load_chunks()
        self.vector_index, self.id_map = self._load_vector_index()
        
        # åˆå§‹åŒ–æ£€ç´¢å™¨
        self.bm25_retriever = BM25Retriever(self.chunks) if self.chunks else None
        self.reranker = SimpleReranker(config)
        
        # åˆå§‹åŒ–å¢å¼ºå›¾æ£€ç´¢å™¨
        try:
            self.graph_retriever = EnhancedGraphRetriever(work_dir, self.llm_client, config)
        except Exception as e:
            print(f"âš ï¸ å›¾æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.graph_retriever = None
        
        # æ£€ç´¢é…ç½®
        retrieval_config = config.get('enhanced_retrieval', {})
        self.vector_top_k = retrieval_config.get('vector_top_k', 10)
        self.bm25_top_k = retrieval_config.get('bm25_top_k', 5)
        self.final_top_k = retrieval_config.get('final_top_k', 5)
        self.use_enhanced_graph = retrieval_config.get('use_enhanced_graph', True)
        self.enable_query_expansion = retrieval_config.get('enable_query_expansion', True)
        self.enable_bm25 = retrieval_config.get('enable_bm25', True)
        self.enable_graph_retrieval = retrieval_config.get('enable_graph_retrieval', True)
        self.enable_reranking = retrieval_config.get('enable_reranking', True)
        
        # åŠ è½½å›¾è°±ç›¸å…³æ•°æ®
        self.graph = None
        self.entity_names = set()
        if self.enable_graph_retrieval:
            self._load_graph_data()
        
    def _load_chunks(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ–‡æ¡£å—"""
        return load_chunks(self.work_dir)
            
    def _load_vector_index(self) -> Tuple[Any, Dict[str, Any]]:
        """åŠ è½½å‘é‡ç´¢å¼•"""
        return load_vector_index(self.work_dir)
            
    def _load_graph_data(self):
        """
        åŠ è½½å›¾è°±æ•°æ®
        """
        try:
            self.graph = load_graph(self.work_dir)
            if self.graph:
                self.entity_names = extract_entity_names(self.graph)
                print(f"âœ… æˆåŠŸåŠ è½½å›¾è°±: {len(self.entity_names)}ä¸ªå®ä½“")
            else:
                print("âš ï¸ å›¾è°±æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å›¾è°±æ£€ç´¢")
                self.entity_names = set()
        except Exception as e:
            print(f"âŒ åŠ è½½å›¾è°±æ•°æ®å¤±è´¥: {e}")
            self.graph = None
            self.entity_names = set()
            
    def retrieve(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:
        """
        å¢å¼ºæ£€ç´¢ä¸»å‡½æ•°
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: æœ€ç»ˆè¿”å›çš„ç»“æœæ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        import time
        retrieve_start = time.time()
        
        if top_k is None:
            top_k = self.final_top_k
            
        all_candidates = []
        timing_info = {}
        
        # 1. æŸ¥è¯¢æ‰©å±•
        expansion_start = time.time()
        queries = [query]
        if self.enable_query_expansion:
            try:
                query_enhancer = get_query_enhancer(self.config)
                queries = query_enhancer.enhance_query(query)
                print(f"ğŸ” æŸ¥è¯¢æ‰©å±•: {len(queries)}ä¸ªæŸ¥è¯¢")
            except Exception as e:
                print(f"æŸ¥è¯¢æ‰©å±•å¤±è´¥: {e}")
        
        timing_info['query_expansion'] = time.time() - expansion_start
        print(f"  â±ï¸ æŸ¥è¯¢æ‰©å±•è€—æ—¶: {timing_info['query_expansion']:.3f}ç§’")
                
        # 2. å‘é‡æ£€ç´¢
        vector_start = time.time()
        vector_candidates = self._vector_retrieval(queries)
        all_candidates.extend(vector_candidates)
        timing_info['vector_retrieval'] = time.time() - vector_start
        print(f"  â±ï¸ å‘é‡æ£€ç´¢è€—æ—¶: {timing_info['vector_retrieval']:.3f}ç§’")
        
        # 3. BM25æ£€ç´¢
        bm25_start = time.time()
        bm25_candidates = []
        if self.enable_bm25 and self.bm25_retriever:
            bm25_candidates = self._bm25_retrieval(queries)
            all_candidates.extend(bm25_candidates)
        timing_info['bm25_retrieval'] = time.time() - bm25_start
        print(f"  â±ï¸ BM25æ£€ç´¢è€—æ—¶: {timing_info['bm25_retrieval']:.3f}ç§’")
            
        # 4. å›¾è°±å®ä½“æ£€ç´¢
        graph_start = time.time()
        graph_candidates = []
        if self.enable_graph_retrieval and self.graph and self.entity_names:
            graph_candidates = self._graph_entity_retrieval(query)
            all_candidates.extend(graph_candidates)
        timing_info['graph_retrieval'] = time.time() - graph_start
        print(f"  â±ï¸ å›¾è°±æ£€ç´¢è€—æ—¶: {timing_info['graph_retrieval']:.3f}ç§’")
            
        # 5. å¢å¼ºå›¾æ£€ç´¢ï¼ˆæ–°å¢ï¼‰
        enhanced_graph_start = time.time()
        enhanced_graph_candidates = []
        if self.use_enhanced_graph and self.graph_retriever:
            enhanced_graph_candidates = self._enhanced_graph_retrieval(query)
            all_candidates.extend(enhanced_graph_candidates)
        timing_info['enhanced_graph_retrieval'] = time.time() - enhanced_graph_start
        print(f"  â±ï¸ å¢å¼ºå›¾æ£€ç´¢è€—æ—¶: {timing_info['enhanced_graph_retrieval']:.3f}ç§’")
            
        # 6. å»é‡åˆå¹¶
        dedup_start = time.time()
        unique_candidates = self._deduplicate_candidates(all_candidates)
        timing_info['deduplication'] = time.time() - dedup_start
        print(f"  â±ï¸ å»é‡åˆå¹¶è€—æ—¶: {timing_info['deduplication']:.3f}ç§’")
        
        # 6.5. å¾—åˆ†å½’ä¸€åŒ–ï¼ˆæ–°å¢ï¼‰
        norm_start = time.time()
        if unique_candidates:
            from utils.common import normalize_scores
            unique_candidates = normalize_scores(unique_candidates, 'similarity', 'minmax')
        timing_info['score_normalization'] = time.time() - norm_start
        print(f"  â±ï¸ å¾—åˆ†å½’ä¸€åŒ–è€—æ—¶: {timing_info['score_normalization']:.3f}ç§’")
        
        # 7. é‡æ’åº
        rerank_start = time.time()
        if unique_candidates and self.enable_reranking:
            final_results = self.reranker.rerank(query, unique_candidates, top_k)
        else:
            # ä½¿ç”¨å½’ä¸€åŒ–åçš„å¾—åˆ†æ’åº
            unique_candidates.sort(key=lambda x: x.get('normalized_similarity', x.get('similarity', 0)), reverse=True)
            final_results = unique_candidates[:top_k] if unique_candidates else []
        timing_info['final_reranking'] = time.time() - rerank_start
        print(f"  â±ï¸ æœ€ç»ˆé‡æ’åºè€—æ—¶: {timing_info['final_reranking']:.3f}ç§’")
        
        total_retrieve_time = time.time() - retrieve_start
        timing_info['total_retrieval'] = total_retrieve_time
        
        print(f"ğŸ“Š æ£€ç´¢ç»Ÿè®¡: å‘é‡{len(vector_candidates)}æ¡, BM25{len(bm25_candidates)}æ¡, å›¾è°±{len(graph_candidates)}æ¡, å¢å¼ºå›¾{len(enhanced_graph_candidates)}æ¡, å»é‡å{len(unique_candidates)}æ¡, æœ€ç»ˆ{len(final_results)}æ¡")
        print(f"ğŸ“Š å¢å¼ºæ£€ç´¢æ€»è€—æ—¶: {total_retrieve_time:.3f}ç§’")
        
        return final_results
        
    def _vector_retrieval(self, queries: List[str]) -> List[Dict[str, Any]]:
        """
        å‘é‡æ£€ç´¢
        """
        if not self.vector_index or not queries:
            return []
            
        all_results = []
        
        for query in queries:
            try:
                # ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå¯ç”¨æ–‡æœ¬é¢„å¤„ç†å’Œç»´åº¦éªŒè¯ï¼‰
                query_embeddings = self.llm_client.embed([query], normalize_text=True, validate_dim=True)
                if not query_embeddings:
                    print(f"æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥: {query}")
                    continue
                    
                query_embedding = query_embeddings[0]
                query_vector = np.array([query_embedding], dtype=np.float32)
                
                # æ£€æŸ¥å‘é‡æœ‰æ•ˆæ€§
                if query_vector.shape[1] == 0:
                    print(f"æŸ¥è¯¢å‘é‡ä¸ºç©º: {query}")
                    continue
                
                # æ£€æŸ¥æŸ¥è¯¢å‘é‡å¹…åº¦ï¼ˆå¼‚å¸¸æ£€æµ‹ï¼‰
                vector_magnitude = np.linalg.norm(query_vector)
                if vector_magnitude < 1e-6:
                    print(f"âš ï¸ æŸ¥è¯¢å‘é‡å¹…åº¦è¿‡å°: {vector_magnitude}, å¯èƒ½å­˜åœ¨å¼‚å¸¸")
                    continue
                    
                # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡ä»¥ç¡®ä¿è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                import faiss
                faiss.normalize_L2(query_vector)
                
                # æ£€æŸ¥å½’ä¸€åŒ–åçš„å‘é‡
                if np.any(np.isnan(query_vector)) or np.any(np.isinf(query_vector)):
                    print(f"æŸ¥è¯¢å‘é‡åŒ…å«æ— æ•ˆå€¼: {query}")
                    continue
                
                # å‘é‡æ£€ç´¢
                similarities, indices = self.vector_index.search(query_vector, self.vector_top_k)
                
                # è½¬æ¢ç»“æœ
                for sim, idx in zip(similarities[0], indices[0]):
                    if idx != -1 and str(idx) in self.id_map:
                        chunk_id = self.id_map[str(idx)]
                        chunk = next((c for c in self.chunks if c['id'] == chunk_id), None)
                        if chunk:
                            # éªŒè¯ç›¸ä¼¼åº¦åˆ†æ•°
                            if not np.isfinite(sim) or sim < -1.1 or sim > 1.1:
                                print(f"å¼‚å¸¸ç›¸ä¼¼åº¦åˆ†æ•°: {sim}, è·³è¿‡ç»“æœ")
                                continue
                                
                            result = chunk.copy()
                            result['similarity'] = float(sim)
                            result['retrieval_type'] = 'vector'
                            result['query_variant'] = query
                            all_results.append(result)
                            
            except Exception as e:
                print(f"å‘é‡æ£€ç´¢å¤±è´¥ (æŸ¥è¯¢: {query}): {e}")
                import traceback
                print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                
        return all_results
        
    def _bm25_retrieval(self, queries: List[str]) -> List[Dict[str, Any]]:
        """
        BM25æ£€ç´¢
        """
        if not self.bm25_retriever or not queries:
            return []
            
        all_results = []
        
        for query in queries:
            try:
                results = self.bm25_retriever.search(query, self.bm25_top_k)
                for result in results:
                    result['query_variant'] = query
                    all_results.append(result)
            except Exception as e:
                print(f"BM25æ£€ç´¢å¤±è´¥ (æŸ¥è¯¢: {query}): {e}")
                
        return all_results
        
    def _graph_entity_retrieval(self, query: str) -> List[Dict[str, Any]]:
        """
        åŸºäºå›¾è°±å®ä½“çš„æ£€ç´¢ï¼Œæ”¯æŒå¤šè·³å›¾æ¨ç†
        """
        if not self.graph or not self.entity_names or not self.chunks:
            return []
            
        try:
            # åŒ¹é…æŸ¥è¯¢ä¸­çš„å®ä½“ï¼ˆä½¿ç”¨æ”¹è¿›çš„åŒ¹é…ç®—æ³•å’Œé…ç½®ï¼‰
            matched_entities = match_entities_in_query(
                query, 
                self.entity_names,
                use_cache=True  # å¯ç”¨ç¼“å­˜ä»¥æé«˜æ€§èƒ½
            )
            
            if not matched_entities:
                return []
                
            print(f"ğŸ” å›¾è°±æ£€ç´¢åŒ¹é…åˆ°å®ä½“: {matched_entities}")
            
            # åŸºäºåŒ¹é…çš„å®ä½“æ£€ç´¢ç›¸å…³æ–‡æ¡£å—
            graph_results = retrieve_by_entity(self.graph, matched_entities, self.chunks)
            
            # å¦‚æœåŒ¹é…åˆ°å¤šä¸ªå®ä½“ï¼Œä½¿ç”¨å¤šè·³å›¾æ¨ç†å¢å¼ºæ£€ç´¢ç»“æœ
            if len(matched_entities) >= 2:
                try:
                    # ç”Ÿæˆå¤šè·³å›¾æ¨ç†æ‘˜è¦
                    multi_hop_summary = get_multi_hop_retrieval_summary(
                        self.graph, matched_entities, self.chunks, max_summary_length=400
                    )
                    
                    print(f"ğŸ“Š å¤šè·³å›¾æ¨ç†æ‘˜è¦: {multi_hop_summary[:100]}...")
                    
                    # å°†æ‘˜è¦ä¿¡æ¯æ·»åŠ åˆ°æ£€ç´¢ç»“æœä¸­
                    for result in graph_results:
                        if 'graph_summary' not in result:
                            result['graph_summary'] = multi_hop_summary
                            result['multi_hop_entities'] = list(matched_entities)
                            
                    # å¯»æ‰¾ä¸­ä»‹å®ä½“è·¯å¾„
                    intermediate_info = find_intermediate_entities(self.graph, matched_entities)
                    if intermediate_info:
                        print(f"ğŸ”— å‘ç°{len(intermediate_info)}æ¡ä¸­ä»‹å®ä½“è·¯å¾„")
                        
                        # å°†ä¸­ä»‹å®ä½“ä¿¡æ¯æ·»åŠ åˆ°ç»“æœä¸­
                        for result in graph_results:
                            result['intermediate_paths'] = intermediate_info[:5]  # é™åˆ¶æ•°é‡
                            
                except Exception as e:
                    print(f"å¤šè·³å›¾æ¨ç†å¤„ç†å¤±è´¥: {e}")
            
            return graph_results
            
        except Exception as e:
            print(f"å›¾è°±å®ä½“æ£€ç´¢å¤±è´¥: {e}")
            return []
        
    def _deduplicate_candidates(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å»é‡åˆå¹¶å€™é€‰ç»“æœï¼Œæ”¯æŒåŸºäºIDå’Œå†…å®¹ç›¸ä¼¼åº¦çš„åŒé‡å»é‡
        """
        if not candidates:
            return []
            
        # ç¬¬ä¸€æ­¥ï¼šæŒ‰æ–‡æ¡£IDåˆ†ç»„å»é‡
        grouped_by_id = defaultdict(list)
        for candidate in candidates:
            doc_id = candidate.get('id', '')
            grouped_by_id[doc_id].append(candidate)
            
        # åˆå¹¶åŒä¸€æ–‡æ¡£çš„å¤šä¸ªæ£€ç´¢ç»“æœ
        id_deduplicated = []
        for doc_id, group in grouped_by_id.items():
            if not group:
                continue
                
            # é€‰æ‹©æœ€é«˜åˆ†æ•°çš„ç»“æœä½œä¸ºä»£è¡¨
            best_candidate = max(group, key=lambda x: x.get('similarity', 0))
            
            # è®°å½•æ‰€æœ‰æ£€ç´¢ç±»å‹
            retrieval_types = list(set(c.get('retrieval_type', '') for c in group))
            best_candidate['retrieval_types'] = retrieval_types
            
            # å¦‚æœåŒæ—¶è¢«å¤šç§æ–¹æ³•æ£€ç´¢åˆ°ï¼Œç»™äºˆé¢å¤–åŠ åˆ†ï¼ˆä½†ä¸è¶…è¿‡1.0ï¼‰
            if len(retrieval_types) > 1:
                current_sim = best_candidate['similarity']
                if 'graph_entity' in retrieval_types:
                    best_candidate['similarity'] = min(1.0, current_sim + 0.1)
                else:
                    best_candidate['similarity'] = min(1.0, current_sim + 0.05)
                
            id_deduplicated.append(best_candidate)
        
        # ç¬¬äºŒæ­¥ï¼šåŸºäºå†…å®¹ç›¸ä¼¼åº¦çš„å»é‡ï¼ˆå¤„ç†ç›¸åŒå†…å®¹ä½†ä¸åŒIDçš„æƒ…å†µï¼‰
        final_candidates = []
        similarity_threshold = 0.95  # å†…å®¹ç›¸ä¼¼åº¦é˜ˆå€¼
        
        for candidate in id_deduplicated:
            is_duplicate = False
            candidate_text = candidate.get('text', '').strip()
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰å€™é€‰ç»“æœå†…å®¹è¿‡äºç›¸ä¼¼
            for existing in final_candidates:
                existing_text = existing.get('text', '').strip()
                
                # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„å­—ç¬¦çº§ç›¸ä¼¼åº¦ï¼‰
                if candidate_text and existing_text:
                    # ä½¿ç”¨Jaccardç›¸ä¼¼åº¦è¿›è¡Œå¿«é€Ÿæ¯”è¾ƒ
                    set1 = set(candidate_text)
                    set2 = set(existing_text)
                    intersection = len(set1 & set2)
                    union = len(set1 | set2)
                    jaccard_sim = intersection / union if union > 0 else 0
                    
                    if jaccard_sim > similarity_threshold:
                        # å†…å®¹è¿‡äºç›¸ä¼¼ï¼Œä¿ç•™å¾—åˆ†æ›´é«˜çš„
                        if candidate.get('similarity', 0) > existing.get('similarity', 0):
                            # æ›¿æ¢ç°æœ‰ç»“æœ
                            final_candidates.remove(existing)
                            final_candidates.append(candidate)
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                final_candidates.append(candidate)
            
        return final_candidates
    
    def _enhanced_graph_retrieval(self, query: str) -> List[Dict[str, Any]]:
        """
        å¢å¼ºå›¾æ£€ç´¢æ–¹æ³•
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not self.graph_retriever:
            return []
            
        try:
            # ä½¿ç”¨å¢å¼ºå›¾æ£€ç´¢å™¨è¿›è¡Œæ£€ç´¢
            retrieval_results = self.graph_retriever.enhanced_entity_retrieval(
                query, 
                top_k=self.vector_top_k,  # ä½¿ç”¨ç›¸åŒçš„top_kè®¾ç½®
                use_vector=True,
                use_matching=True
            )
            
            candidates = []
            
            # å¤„ç†æ£€ç´¢åˆ°çš„å®ä½“
            combined_entities = retrieval_results.get('combined_entities', [])
            entity_neighborhoods = retrieval_results.get('entity_neighborhoods', {})
            
            for entity, score in combined_entities:
                # æŸ¥æ‰¾åŒ…å«è¯¥å®ä½“çš„æ–‡æ¡£å—
                related_chunks = self._find_chunks_by_entity(entity)
                
                for chunk in related_chunks:
                    result = chunk.copy()
                    result['similarity'] = float(score * 0.8)  # è°ƒæ•´åˆ†æ•°æƒé‡
                    result['retrieval_type'] = 'enhanced_graph'
                    result['matched_entity'] = entity
                    
                    # æ·»åŠ å®ä½“é‚»åŸŸä¿¡æ¯
                    if entity in entity_neighborhoods:
                        neighborhood = entity_neighborhoods[entity]
                        result['entity_context'] = {
                            'neighbors_count': neighborhood.get('subgraph_size', 0),
                            'relations_count': len(neighborhood.get('relations', []))
                        }
                    
                    candidates.append(result)
            
            # å¤„ç†å®ä½“è·¯å¾„ä¿¡æ¯
            entity_paths = retrieval_results.get('entity_paths', [])
            if entity_paths:
                # ä¸ºåŒ…å«è·¯å¾„å®ä½“çš„æ–‡æ¡£å—å¢åŠ é¢å¤–åˆ†æ•°
                path_entities = set()
                for path_info in entity_paths:
                    path_entities.update(path_info.get('path', []))
                
                for candidate in candidates:
                    matched_entity = candidate.get('matched_entity', '')
                    if matched_entity in path_entities:
                        candidate['similarity'] = min(1.0, candidate['similarity'] + 0.1)
                        candidate['has_entity_path'] = True
            
            print(f"ğŸ”— å¢å¼ºå›¾æ£€ç´¢: æ‰¾åˆ° {len(combined_entities)} ä¸ªç›¸å…³å®ä½“, {len(candidates)} ä¸ªå€™é€‰æ–‡æ¡£")
            return candidates
            
        except Exception as e:
            print(f"âš ï¸ å¢å¼ºå›¾æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _find_chunks_by_entity(self, entity: str) -> List[Dict[str, Any]]:
        """
        æ ¹æ®å®ä½“æŸ¥æ‰¾ç›¸å…³çš„æ–‡æ¡£å—
        
        Args:
            entity: å®ä½“åç§°
            
        Returns:
            ç›¸å…³æ–‡æ¡£å—åˆ—è¡¨
        """
        if not self.chunks:
            return []
            
        related_chunks = []
        
        # ç®€å•çš„æ–‡æœ¬åŒ¹é…æŸ¥æ‰¾
        for chunk in self.chunks:
            chunk_text = chunk.get('text', '').lower()
            if entity.lower() in chunk_text:
                related_chunks.append(chunk)
        
        return related_chunks