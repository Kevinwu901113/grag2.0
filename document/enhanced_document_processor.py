import os
import json
import sys
from typing import List, Dict
from tqdm import tqdm
from docx import Document
from llm.llm import LLMClient
# ç§»é™¤èšç±»åŠŸèƒ½ï¼Œä¿ç•™ä¸»é¢˜æ± æ¦‚å¿µ
from document.redundancy_buffer import RedundancyBuffer, EnhancedRedundancyBuffer
from document.sentence_splitter import split_into_sentences
from utils.config_manager import ConfigManager
from redundancy.redundancy_filter_factory import RedundancyFilterFactory, create_redundancy_filter
from utils.performance_monitor import performance_monitor

def read_docx(file_path: str) -> List[str]:
    """è¯»å–DOCXæ–‡ä»¶å†…å®¹"""
    doc = Document(file_path)
    return [para.text.strip() for para in doc.paragraphs if para.text.strip()]

def read_json(file_path: str) -> List[str]:
    """è¯»å–JSONæ–‡ä»¶å†…å®¹"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        if isinstance(data, list):
            return [item.get("content", "") for item in data if isinstance(item, dict) and "content" in item]
        return []

def read_jsonl(file_path: str) -> List[str]:
    """è¯»å–JSONLæ–‡ä»¶å†…å®¹"""
    paragraphs = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    data = json.loads(line)
                    if isinstance(data, dict):
                        # æ”¯æŒå¤šç§å¸¸è§çš„å†…å®¹å­—æ®µå
                        content = (
                            data.get("content") or 
                            data.get("text") or 
                            data.get("body") or 
                            data.get("message") or
                            data.get("question") or
                            data.get("answer")
                        )
                        if content and isinstance(content, str):
                            paragraphs.append(content.strip())
                        
                        # å¤„ç†paragraphså­—æ®µï¼ˆå¦‚musiqueæ•°æ®é›†ï¼‰
                        if "paragraphs" in data and isinstance(data["paragraphs"], list):
                            for para in data["paragraphs"]:
                                if isinstance(para, dict) and "paragraph_text" in para:
                                    para_text = para["paragraph_text"]
                                    if para_text and isinstance(para_text, str):
                                        paragraphs.append(para_text.strip())
                except json.JSONDecodeError:
                    continue
    return paragraphs

def read_txt(file_path: str) -> List[str]:
    """è¯»å–TXTæ–‡ä»¶å†…å®¹"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        return paragraphs if paragraphs else [content]

class EnhancedDocumentProcessor:
    """
    å¢å¼ºçš„æ–‡æ¡£å¤„ç†å™¨ï¼šé›†æˆé«˜çº§èšç±»åŠŸèƒ½
    æ”¯æŒå¥å­çº§åˆ†è§£å’Œå¤šç§èšç±»ç®—æ³•
    """
    
    def __init__(self, config: dict):
        self.config = config
        self.llm_client = LLMClient(config)
        
        # ä½¿ç”¨æ–°çš„é…ç½®ç®¡ç†å™¨
        if isinstance(config, dict) and 'config_path' in config:
            self.config_manager = ConfigManager(config['config_path'])
        else:
            # å…¼å®¹æ—§çš„é…ç½®æ–¹å¼
            self.config_manager = None
        
        # å†—ä½™è¿‡æ»¤å™¨é…ç½® - ä½¿ç”¨æ–°çš„å·¥å‚æ¨¡å¼
        try:
            if self.config_manager:
                # ä½¿ç”¨æ–°çš„é…ç½®ç®¡ç†å™¨å’Œå·¥å‚æ¨¡å¼
                self.redundancy_filter = RedundancyFilterFactory.create_from_config_manager(self.config_manager)
            else:
                # å…¼å®¹æ—§çš„é…ç½®æ–¹å¼
                redundancy_config = config.get("redundancy", config.get("redundancy_filter", {}))
                
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–°çš„å†—ä½™é…ç½®ç»“æ„
                if 'method' in redundancy_config:
                    # æ–°çš„é…ç½®ç»“æ„
                    self.redundancy_filter = RedundancyFilterFactory.create_filter(redundancy_config)
                else:
                    # æ—§çš„é…ç½®ç»“æ„ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
                    enable_enhanced_filter = redundancy_config.get("enable_enhanced_filter", False)
                    
                    if enable_enhanced_filter:
                        # ä½¿ç”¨å¢å¼ºå†—ä½™è¿‡æ»¤å™¨
                        self.redundancy_filter = EnhancedRedundancyBuffer(redundancy_config)
                    else:
                        # ä½¿ç”¨ä¼ ç»Ÿå†—ä½™è¿‡æ»¤å™¨
                        redundancy_threshold = config["document"].get("redundancy_threshold", 0.95)
                        redundancy_config = {
                            'threshold': redundancy_threshold,
                            'enable_logging': True,
                            'enable_progress': True
                        }
                        self.redundancy_filter = RedundancyBuffer(config=redundancy_config)
        except Exception as e:
            # å¦‚æœæ–°æ–¹å¼å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
            print(f"è­¦å‘Šï¼šä½¿ç”¨æ–°çš„å†—ä½™è¿‡æ»¤å™¨å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹å¼: {e}")
            redundancy_threshold = config["document"].get("redundancy_threshold", 0.95)
            redundancy_config = {
                'threshold': redundancy_threshold,
                'enable_logging': True,
                'enable_progress': True
            }
            self.redundancy_filter = RedundancyBuffer(config=redundancy_config)
        
        # å¤„ç†æ¨¡å¼é…ç½® - ç§»é™¤é«˜çº§èšç±»ï¼Œä»…æ”¯æŒä¼ ç»Ÿæ¨¡å¼
        self.processing_mode = "traditional"
        
    @performance_monitor()
    def process_documents(self, input_dir: str, work_dir: str, logger) -> Dict:
        """
        å¤„ç†æ–‡æ¡£ç›®å½•ï¼Œè¿”å›å¤„ç†ç»“æœ
        
        Args:
            input_dir: è¾“å…¥æ–‡æ¡£ç›®å½•
            work_dir: å·¥ä½œç›®å½•
            logger: æ—¥å¿—è®°å½•å™¨
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        allowed_types = self.config["document"].get("allowed_types", [".docx", ".json", ".jsonl", ".txt"])
        
        logger.info(f"ä½¿ç”¨å¤„ç†æ¨¡å¼: {self.processing_mode}")
        
        # è¯»å–æ‰€æœ‰æ–‡æ¡£
        documents = self._load_documents(input_dir, allowed_types, logger)
        
        if not documents:
            logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£")
            return {"topics": [], "stats": {}}
        
        logger.info(f"å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        
        # åº”ç”¨å†—ä½™è¿‡æ»¤
        filtered_documents = self._apply_redundancy_filter(documents, logger)
        
        # ä½¿ç”¨ä¼ ç»Ÿä¸»é¢˜æ± å¤„ç†æ–¹å¼
        topics = self._process_with_traditional_method(filtered_documents, logger)
        
        # ä¿å­˜ç»“æœ
        logger.info("å¼€å§‹ä¿å­˜å¤„ç†ç»“æœ...")
        results = self._save_results(topics, work_dir, logger)
        logger.info("ç»“æœä¿å­˜å®Œæˆ")
        
        return results
    
    @performance_monitor()
    def _load_documents(self, input_dir: str, allowed_types: List[str], logger) -> List[Dict]:
        """
        åŠ è½½æ–‡æ¡£ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æ¡£
        
        Args:
            input_dir: è¾“å…¥ç›®å½•
            allowed_types: å…è®¸çš„æ–‡ä»¶ç±»å‹
            logger: æ—¥å¿—è®°å½•å™¨
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        documents = []
        doc_id = 0
        
        for filename in os.listdir(input_dir):
            ext = os.path.splitext(filename)[-1].lower()
            if ext not in allowed_types:
                continue
                
            file_path = os.path.join(input_dir, filename)
            
            try:
                logger.info(f"æ­£åœ¨åŠ è½½æ–‡ä»¶: {filename}")
                
                if ext == ".docx":
                    paragraphs = read_docx(file_path)
                elif ext == ".json":
                    paragraphs = read_json(file_path)
                elif ext == ".jsonl":
                    paragraphs = read_jsonl(file_path)
                elif ext == ".txt":
                    paragraphs = read_txt(file_path)
                else:
                    continue
                
                # å°†æ®µè½åˆå¹¶ä¸ºæ–‡æ¡£
                full_text = "\n".join(paragraphs)
                
                if full_text.strip():
                    documents.append({
                        "id": f"doc_{doc_id}",
                        "text": full_text,
                        "meta": {
                            "source": filename,
                            "file_path": file_path,
                            "paragraph_count": len(paragraphs)
                        }
                    })
                    doc_id += 1
                    
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {str(e)}")
                
        return documents
    
    @performance_monitor()
    def _apply_redundancy_filter(self, documents: List[Dict], logger) -> List[Dict]:
        """
        åº”ç”¨å†—ä½™è¿‡æ»¤
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            logger: æ—¥å¿—è®°å½•å™¨
            
        Returns:
            è¿‡æ»¤åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.config["document"].get("enable_redundancy_filter", True):
            return documents
            
        filtered_documents = []
        
        for doc in documents:
            text = doc["text"]
            
            # å¯¹äºå¥å­çº§å¤„ç†ï¼Œéœ€è¦æ£€æŸ¥æ¯ä¸ªå¥å­
            if self.config["document"].get("sentence_level", True):
                sentences = split_into_sentences(text)
                filtered_sentences = []
                
                min_sentence_length = self.config["document"].get("min_sentence_length", 10)
                # è¿‡æ»¤æœ‰æ•ˆå¥å­
                valid_sentences = [(i, sentence) for i, sentence in enumerate(sentences) 
                                 if len(sentence.strip()) >= min_sentence_length]
                
                if valid_sentences:
                    # ä¼˜åŒ–çš„æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡ - æ”¯æŒHuggingFaceæ‰¹é‡å¤„ç†
                    batch_size = self.config["document"].get("embedding_batch_size", 64)
                    sentence_texts = [sentence for _, sentence in valid_sentences]
                    
                    try:
                        # ä¸€æ¬¡æ€§æ‰¹é‡å¤„ç†æ‰€æœ‰å¥å­ï¼Œæé«˜HuggingFaceæ¨¡å‹æ•ˆç‡
                        all_embeddings = self.llm_client.embed(sentence_texts)
                        logger.debug(f"æ‰¹é‡åµŒå…¥æˆåŠŸ: {len(all_embeddings)} ä¸ªå¥å­")
                    except Exception as e:
                        logger.warning(f"æ‰¹é‡åµŒå…¥å¤±è´¥ï¼Œå›é€€åˆ°åˆ†æ‰¹å¤„ç†: {e}")
                        # å›é€€åˆ°åˆ†æ‰¹å¤„ç†
                        all_embeddings = []
                        for i in range(0, len(sentence_texts), batch_size):
                            batch_texts = sentence_texts[i:i+batch_size]
                            try:
                                batch_embeddings = self.llm_client.embed(batch_texts)
                                all_embeddings.extend(batch_embeddings)
                            except Exception as batch_e:
                                logger.warning(f"åˆ†æ‰¹åµŒå…¥å¤±è´¥ï¼Œå›é€€åˆ°å•å¥å¤„ç†: {batch_e}")
                                # æœ€åå›é€€åˆ°å•å¥å¤„ç†
                                for sentence in batch_texts:
                                    try:
                                        embedding = self.llm_client.embed([sentence])[0]
                                        all_embeddings.append(embedding)
                                    except Exception:
                                        # ä½¿ç”¨é›¶å‘é‡ä½œä¸ºå ä½ç¬¦
                                        embedding_dim = self.config.get("embedding", {}).get("dimension", 768)
                                        all_embeddings.append([0.0] * embedding_dim)
                    
                    # é€å¥è¿›è¡Œå†—ä½™æ£€æµ‹
                    for (original_idx, sentence), embedding in zip(valid_sentences, all_embeddings):
                        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¢å¼ºå†—ä½™è¿‡æ»¤å™¨
                        if hasattr(self.redundancy_filter, 'is_redundant_enhanced'):
                            # è·å–ä¸Šä¸‹æ–‡
                            context_before = sentences[original_idx-1] if original_idx > 0 else ""
                            context_after = sentences[original_idx+1] if original_idx < len(sentences)-1 else ""
                            is_redundant = self.redundancy_filter.is_redundant_enhanced(
                                sentence, embedding, context_before, context_after
                            )
                        else:
                            is_redundant = self.redundancy_filter.is_redundant(sentence, embedding)
                            
                        if not is_redundant:
                            filtered_sentences.append(sentence)
                
                if filtered_sentences:
                    filtered_doc = doc.copy()
                    filtered_doc["text"] = "\n".join(filtered_sentences)
                    filtered_doc["meta"]["original_sentence_count"] = len(sentences)
                    filtered_doc["meta"]["filtered_sentence_count"] = len(filtered_sentences)
                    filtered_documents.append(filtered_doc)
            else:
                # æ–‡æ¡£çº§å†—ä½™æ£€æµ‹
                embedding = self.llm_client.embed([text])[0]
                
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¢å¼ºå†—ä½™è¿‡æ»¤å™¨
                if hasattr(self.redundancy_filter, 'is_redundant_enhanced'):
                    is_redundant = self.redundancy_filter.is_redundant_enhanced(text, embedding)
                else:
                    is_redundant = self.redundancy_filter.is_redundant(text, embedding)
                    
                if not is_redundant:
                    filtered_documents.append(doc)
        
        logger.info(f"å†—ä½™è¿‡æ»¤åä¿ç•™ {len(filtered_documents)} ä¸ªæ–‡æ¡£")
        return filtered_documents
    
    # ç§»é™¤é«˜çº§èšç±»å¤„ç†æ–¹æ³•
    
    @performance_monitor()
    def _process_with_traditional_method(self, documents: List[Dict], logger) -> List[Dict]:
        """
        ä½¿ç”¨ä¼˜åŒ–çš„ä¼ ç»Ÿä¸»é¢˜æ± æ–¹æ³•å¤„ç†æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            logger: æ—¥å¿—è®°å½•å™¨
            
        Returns:
            ä¸»é¢˜åˆ—è¡¨
        """
        logger.info("ä½¿ç”¨ä¼˜åŒ–çš„ä¼ ç»Ÿå¤„ç†æ–¹æ³•...")
        
        # å¯¼å…¥ä¼ ç»Ÿçš„ä¸»é¢˜æ± ç®¡ç†å™¨
        from document.topic_pool_manager import TopicPoolManager
        
        sim_threshold = self.config["document"].get("similarity_threshold", 0.80)
        
        # åˆå§‹åŒ–ä¼˜åŒ–çš„ä¸»é¢˜æ± ç®¡ç†å™¨
        topic_manager = TopicPoolManager(
            similarity_threshold=sim_threshold,
            redundancy_filter=self.redundancy_filter,
            config=self.config
        )
        
        # æ‰¹é‡åµŒå…¥é…ç½®
        batch_size = self.config["document"].get("embedding_batch_size", 32)
        min_sentence_length = self.config["document"].get("min_sentence_length", 10)
        enable_parallel_docs = self.config.get("topic_pool", {}).get("enable_parallel_document_processing", False)
        
        total_sentences = 0
        
        if enable_parallel_docs and len(documents) > 1:
            # å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æ¡£ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰
            logger.info(f"å¯ç”¨å¹¶è¡Œæ–‡æ¡£å¤„ç†ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
            total_sentences = self._process_documents_parallel(documents, topic_manager, logger)
        else:
            # é¡ºåºå¤„ç†æ–‡æ¡£
            # é¦–å…ˆè®¡ç®—æ€»çš„å¤„ç†å•å…ƒæ•°é‡ï¼ˆå¥å­æˆ–æ–‡æ¡£ï¼‰
            total_processing_units = 0
            for doc in documents:
                if self.config["document"].get("sentence_level_traditional", False):
                    sentences = split_into_sentences(doc["text"])
                    valid_sentences = [s for s in sentences if len(s.strip()) >= min_sentence_length]
                    total_processing_units += len(valid_sentences)
                else:
                    total_processing_units += 1
            
            logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£ï¼Œé¢„è®¡å¤„ç† {total_processing_units} ä¸ªæ–‡æœ¬å—")
            
            # ä½¿ç”¨æ—¥å¿—ç³»ç»Ÿæ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
            logger.info(f"å¼€å§‹å¤„ç† {total_processing_units} ä¸ªæ–‡æœ¬å—...")
            processed_units = 0
            last_logged_percentage = -1
            
            # ä½¿ç”¨æ—¥å¿—æ˜¾ç¤ºè¿›åº¦
            def update_progress(increment=1):
                nonlocal processed_units, last_logged_percentage
                processed_units += increment
                percentage = int((processed_units / total_processing_units) * 100)
                # æ¯10%è®°å½•ä¸€æ¬¡è¿›åº¦ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                if percentage >= last_logged_percentage + 10 or processed_units == total_processing_units:
                    logger.info(f"ğŸ“Š å¤„ç†è¿›åº¦: {processed_units}/{total_processing_units} ({percentage}%)")
                    last_logged_percentage = percentage
            
            # åˆ›å»ºè¿›åº¦æ¡å¯¹è±¡ç”¨äºå…¼å®¹ç°æœ‰ä»£ç 
            class LogProgressBar:
                def update(self, n=1):
                    update_progress(n)
                def refresh(self):
                    pass
                def __enter__(self):
                    return self
                def __exit__(self, *args):
                    logger.info(f"âœ… å¤„ç†å®Œæˆ: {processed_units}/{total_processing_units} (100%)")
            
            with LogProgressBar() as pbar:
                for doc_idx, doc in enumerate(documents):
                    logger.info(f"å¤„ç†æ–‡æ¡£ {doc_idx + 1}/{len(documents)}: {doc['meta'].get('source', 'Unknown')}")
                    
                    text = doc["text"]
                    meta = doc["meta"]
                    
                    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦è¿›è¡Œå¥å­çº§åˆ†è§£
                    if self.config["document"].get("sentence_level_traditional", False):
                        sentences = split_into_sentences(text)
                        # è¿‡æ»¤æœ‰æ•ˆå¥å­
                        valid_sentences = [s for s in sentences if len(s.strip()) >= min_sentence_length]
                        
                        if valid_sentences:
                            # ä¸ºæ¯ä¸ªå¥å­å‡†å¤‡ç›¸åŒçš„å…ƒæ•°æ®
                            sentence_metas = [meta] * len(valid_sentences)
                            # ä½¿ç”¨æ‰¹é‡å¤„ç†æ¥å£ï¼Œå¹¶ä¼ é€’è¿›åº¦æ¡å›è°ƒ
                            topic_manager.add_sentences_batch_with_progress(
                                valid_sentences, sentence_metas, batch_size, pbar
                            )
                            total_sentences += len(valid_sentences)
                            # å¼ºåˆ¶åˆ·æ–°è¿›åº¦æ¡æ˜¾ç¤º
                            pbar.refresh()
                    else:
                        # æ–‡æ¡£çº§å¤„ç†ï¼Œä»ä½¿ç”¨å•å¥æ¥å£
                        topic_manager.add_sentence(text, meta)
                        total_sentences += 1
                        pbar.update(1)
                        # å¼ºåˆ¶åˆ·æ–°è¿›åº¦æ¡æ˜¾ç¤º
                        pbar.refresh()
        
        logger.info(f"ä¼˜åŒ–ä¼ ç»Ÿæ–¹æ³•å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {total_sentences} ä¸ªå¥å­")
        
        # è·å–ä¸»é¢˜æ± ç»Ÿè®¡ä¿¡æ¯
        if hasattr(topic_manager, 'get_topic_pool_stats'):
            stats = topic_manager.get_topic_pool_stats()
            logger.info(f"ä¸»é¢˜æ± ç»Ÿè®¡: {stats}")
        
        # ä¼˜åŒ–ä¸»é¢˜æ± 
        if hasattr(topic_manager, 'optimize_topic_pool'):
            logger.info("å¼€å§‹ä¼˜åŒ–ä¸»é¢˜æ± ...")
            topic_manager.optimize_topic_pool()
            logger.info("ä¸»é¢˜æ± ä¼˜åŒ–å®Œæˆ")
        
        # è·å–ä¸»é¢˜ï¼ˆä¸ç”Ÿæˆæ‘˜è¦ä»¥é¿å…å¤§é‡LLMè°ƒç”¨å¯¼è‡´hangï¼‰
        logger.info("æ­£åœ¨è·å–ä¸»é¢˜åˆ—è¡¨...")
        topics = topic_manager.get_all_topics(llm_client=None)
        logger.info(f"ä¸»é¢˜åˆ—è¡¨è·å–å®Œæˆï¼Œå…± {len(topics)} ä¸ªä¸»é¢˜")
        
        logger.info(f"ä¼ ç»Ÿæ–¹æ³•å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(topics)} ä¸ªä¸»é¢˜")
        return topics
        
    def _process_documents_parallel(self, documents: List[Dict], topic_manager, logger) -> int:
        """
        å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æ¡£ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            topic_manager: ä¸»é¢˜æ± ç®¡ç†å™¨
            logger: æ—¥å¿—è®°å½•å™¨
            
        Returns:
            å¤„ç†çš„å¥å­æ€»æ•°
        """
        import concurrent.futures
        from document.sentence_splitter import split_into_sentences
        
        min_sentence_length = self.config["document"].get("min_sentence_length", 10)
        batch_size = self.config["document"].get("embedding_batch_size", 32)
        max_workers = self.config.get("topic_pool", {}).get("max_workers", 4)
        
        def process_single_document(doc_data):
            doc_idx, doc = doc_data
            text = doc["text"]
            meta = doc["meta"]
            
            if self.config["document"].get("sentence_level_traditional", False):
                sentences = split_into_sentences(text)
                valid_sentences = [s for s in sentences if len(s.strip()) >= min_sentence_length]
                return valid_sentences, [meta] * len(valid_sentences)
            else:
                return [text], [meta]
        
        total_sentences = 0
        
        # å¹¶è¡Œå¤„ç†æ–‡æ¡£
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_doc = {executor.submit(process_single_document, (idx, doc)): idx 
                           for idx, doc in enumerate(documents)}
            
            for future in concurrent.futures.as_completed(future_to_doc):
                doc_idx = future_to_doc[future]
                try:
                    sentences, metas = future.result()
                    if sentences:
                        topic_manager.add_sentences_batch(sentences, metas, batch_size)
                        total_sentences += len(sentences)
                        logger.info(f"å¹¶è¡Œå¤„ç†å®Œæˆæ–‡æ¡£ {doc_idx + 1}ï¼Œå¥å­æ•°: {len(sentences)}")
                except Exception as exc:
                    logger.error(f"æ–‡æ¡£ {doc_idx + 1} å¤„ç†å‡ºé”™: {exc}")
        
        return total_sentences
    
    def _save_results(self, topics: List[Dict], work_dir: str, logger) -> Dict:
        """
        ä¿å­˜å¤„ç†ç»“æœ
        
        Args:
            topics: ä¸»é¢˜åˆ—è¡¨
            work_dir: å·¥ä½œç›®å½•
            logger: æ—¥å¿—è®°å½•å™¨
            
        Returns:
            ç»“æœç»Ÿè®¡ä¿¡æ¯
        """
        # ä¿å­˜ä¸»é¢˜å—
        chunks_path = os.path.join(work_dir, "enhanced_chunks.json")
        with open(chunks_path, 'w', encoding='utf-8') as f:
            json.dump(topics, f, ensure_ascii=False, indent=2)
        logger.info(f"ä¸»é¢˜å—å·²ä¿å­˜è‡³: {chunks_path}")
        
        # ä¿å­˜å†—ä½™å¥æ—¥å¿—
        redundant_log_path = os.path.join(work_dir, "redundant_sentences.json")
        redundant_log = self.redundancy_filter.get_redundant_log()
        with open(redundant_log_path, 'w', encoding='utf-8') as f:
            json.dump(redundant_log, f, ensure_ascii=False, indent=2)
        logger.info(f"å†—ä½™å¥æ—¥å¿—å·²ä¿å­˜è‡³: {redundant_log_path}")
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_topics": len(topics),
            "total_redundant_sentences": len(redundant_log),
            "processing_mode": self.processing_mode,
            "clustering_method": "traditional_topic_pool",
            "topics_by_size": self._analyze_topic_sizes(topics),
            "average_topic_length": sum(len(topic["text"]) for topic in topics) / len(topics) if topics else 0
        }
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_path = os.path.join(work_dir, "processing_stats.json")
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜è‡³: {stats_path}")
        
        return {
            "topics": topics,
            "stats": stats,
            "paths": {
                "chunks": chunks_path,
                "redundant_log": redundant_log_path,
                "stats": stats_path
            }
        }
    
    def _analyze_topic_sizes(self, topics: List[Dict]) -> Dict:
        """
        åˆ†æä¸»é¢˜å¤§å°åˆ†å¸ƒ
        
        Args:
            topics: ä¸»é¢˜åˆ—è¡¨
            
        Returns:
            å¤§å°åˆ†å¸ƒç»Ÿè®¡
        """
        if not topics:
            return {}
            
        sizes = [len(topic["text"]) for topic in topics]
        sentence_counts = [topic.get("sentence_count", 0) for topic in topics]
        
        return {
            "min_length": min(sizes),
            "max_length": max(sizes),
            "avg_length": sum(sizes) / len(sizes),
            "min_sentences": min(sentence_counts) if sentence_counts else 0,
            "max_sentences": max(sentence_counts) if sentence_counts else 0,
            "avg_sentences": sum(sentence_counts) / len(sentence_counts) if sentence_counts else 0
        }

def run_enhanced_document_processing(config: dict, work_dir: str, logger):
    """
    è¿è¡Œå¢å¼ºçš„æ–‡æ¡£å¤„ç†æµç¨‹
    
    æ ¹æ®é…ç½®ä¸­çš„ document_processing.strategy é€‰æ‹©å¤„ç†ç­–ç•¥ï¼š
    - "clustered": ä½¿ç”¨æ–°çš„é™æ€æ‰¹é‡èšç±»å¤„ç†
    - "incremental": ä½¿ç”¨åŸæœ‰çš„ä¸»é¢˜æ± å¢é‡å¤„ç†ï¼ˆé»˜è®¤ï¼‰
    
    Args:
        config: é…ç½®å­—å…¸
        work_dir: å·¥ä½œç›®å½•
        logger: æ—¥å¿—è®°å½•å™¨
        
    Returns:
        å¤„ç†ç»“æœ
    """
    # æ£€æŸ¥å¤„ç†ç­–ç•¥
    strategy = config.get("document_processing", {}).get("strategy", "incremental")
    
    if strategy == "clustered":
        # ä½¿ç”¨æ–°çš„é™æ€æ‰¹é‡èšç±»å¤„ç†
        logger.info("ä½¿ç”¨é™æ€æ‰¹é‡èšç±»å¤„ç†ç­–ç•¥")
        from .static_chunk_processor import run_static_chunk_processing
        return run_static_chunk_processing(config, work_dir, logger)
    else:
        # ä½¿ç”¨åŸæœ‰çš„ä¸»é¢˜æ± å¢é‡å¤„ç†
        logger.info("ä½¿ç”¨ä¼ ç»Ÿä¸»é¢˜æ± å¢é‡å¤„ç†ç­–ç•¥")
        input_dir = config["document"]["input_dir"]
        
        # åˆ›å»ºå¢å¼ºæ–‡æ¡£å¤„ç†å™¨
        processor = EnhancedDocumentProcessor(config)
        
        # æ‰§è¡Œå¤„ç†
        results = processor.process_documents(input_dir, work_dir, logger)
        
        logger.info(f"å¢å¼ºæ–‡æ¡£å¤„ç†å®Œæˆ: {results['stats']}")
        
        return results